{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zeANMo5VPYYo"
      },
      "source": [
        "# **VET-ASSISTANT LLM**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zu96hl0nOwUk"
      },
      "source": [
        "**Installing dependencies**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Un-Qup9eNRq3",
        "outputId": "c8c2d3ae-fbd6-43d4-d688-092572d03f85"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting gradientai\n",
            "  Downloading gradientai-1.13.0-py3-none-any.whl (397 kB)\n",
            "     ------------------------------------ 397.7/397.7 kB 516.7 kB/s eta 0:00:00\n",
            "Collecting aenum>=3.1.11\n",
            "  Using cached aenum-3.1.15-py3-none-any.whl (137 kB)\n",
            "Requirement already satisfied: urllib3>=1.25.3 in c:\\users\\swafey\\anaconda3\\lib\\site-packages (from gradientai) (1.26.14)\n",
            "Collecting pydantic<3,>=1.10.15\n",
            "  Downloading pydantic-2.7.3-py3-none-any.whl (409 kB)\n",
            "     ------------------------------------ 409.6/409.6 kB 709.3 kB/s eta 0:00:00\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\swafey\\anaconda3\\lib\\site-packages (from gradientai) (2.8.2)\n",
            "Collecting annotated-types>=0.4.0\n",
            "  Downloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
            "Collecting typing-extensions>=4.6.1\n",
            "  Downloading typing_extensions-4.12.2-py3-none-any.whl (37 kB)\n",
            "Collecting pydantic-core==2.18.4\n",
            "  Downloading pydantic_core-2.18.4-cp310-none-win_amd64.whl (1.9 MB)\n",
            "     ---------------------------------------- 1.9/1.9 MB 286.4 kB/s eta 0:00:00\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\swafey\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->gradientai) (1.16.0)\n",
            "Installing collected packages: aenum, typing-extensions, annotated-types, pydantic-core, pydantic, gradientai\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing_extensions 4.4.0\n",
            "    Uninstalling typing_extensions-4.4.0:\n",
            "      Successfully uninstalled typing_extensions-4.4.0\n",
            "Successfully installed aenum-3.1.15 annotated-types-0.7.0 gradientai-1.13.0 pydantic-2.7.3 pydantic-core-2.18.4 typing-extensions-4.12.2\n",
            "Collecting langchain\n",
            "  Downloading langchain-0.2.3-py3-none-any.whl (974 kB)\n",
            "     ------------------------------------ 974.0/974.0 kB 295.2 kB/s eta 0:00:00\n",
            "Collecting tenacity<9.0.0,>=8.1.0\n",
            "  Downloading tenacity-8.3.0-py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\swafey\\anaconda3\\lib\\site-packages (from langchain) (1.4.39)\n",
            "Collecting langsmith<0.2.0,>=0.1.17\n",
            "  Downloading langsmith-0.1.76-py3-none-any.whl (124 kB)\n",
            "     -------------------------------------- 124.9/124.9 kB 2.4 MB/s eta 0:00:00\n",
            "Requirement already satisfied: requests<3,>=2 in c:\\users\\swafey\\anaconda3\\lib\\site-packages (from langchain) (2.28.1)\n",
            "Collecting async-timeout<5.0.0,>=4.0.0\n",
            "  Downloading async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
            "Collecting langchain-core<0.3.0,>=0.2.0\n",
            "  Downloading langchain_core-0.2.5-py3-none-any.whl (314 kB)\n",
            "     ------------------------------------ 314.7/314.7 kB 218.9 kB/s eta 0:00:00\n",
            "Collecting aiohttp<4.0.0,>=3.8.3\n",
            "  Downloading aiohttp-3.9.5-cp310-cp310-win_amd64.whl (370 kB)\n",
            "     ------------------------------------ 370.7/370.7 kB 720.9 kB/s eta 0:00:00\n",
            "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\swafey\\anaconda3\\lib\\site-packages (from langchain) (6.0)\n",
            "Requirement already satisfied: numpy<2,>=1 in c:\\users\\swafey\\anaconda3\\lib\\site-packages (from langchain) (1.23.5)\n",
            "Collecting langchain-text-splitters<0.3.0,>=0.2.0\n",
            "  Downloading langchain_text_splitters-0.2.1-py3-none-any.whl (23 kB)\n",
            "Requirement already satisfied: pydantic<3,>=1 in c:\\users\\swafey\\anaconda3\\lib\\site-packages (from langchain) (2.7.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\swafey\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (22.1.0)\n",
            "Collecting aiosignal>=1.1.2\n",
            "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
            "Collecting multidict<7.0,>=4.5\n",
            "  Downloading multidict-6.0.5-cp310-cp310-win_amd64.whl (28 kB)\n",
            "Collecting frozenlist>=1.1.1\n",
            "  Downloading frozenlist-1.4.1-cp310-cp310-win_amd64.whl (50 kB)\n",
            "     --------------------------------------- 50.4/50.4 kB 88.7 kB/s eta 0:00:00\n",
            "Collecting yarl<2.0,>=1.0\n",
            "  Downloading yarl-1.9.4-cp310-cp310-win_amd64.whl (76 kB)\n",
            "     -------------------------------------- 76.4/76.4 kB 141.1 kB/s eta 0:00:00\n",
            "Collecting jsonpatch<2.0,>=1.33\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Collecting packaging<24.0,>=23.2\n",
            "  Downloading packaging-23.2-py3-none-any.whl (53 kB)\n",
            "     -------------------------------------- 53.0/53.0 kB 342.2 kB/s eta 0:00:00\n",
            "Collecting orjson<4.0.0,>=3.9.14\n",
            "  Downloading orjson-3.10.4-cp310-none-win_amd64.whl (139 kB)\n",
            "     ------------------------------------ 139.0/139.0 kB 748.7 kB/s eta 0:00:00\n",
            "Requirement already satisfied: pydantic-core==2.18.4 in c:\\users\\swafey\\anaconda3\\lib\\site-packages (from pydantic<3,>=1->langchain) (2.18.4)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in c:\\users\\swafey\\anaconda3\\lib\\site-packages (from pydantic<3,>=1->langchain) (4.12.2)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\swafey\\anaconda3\\lib\\site-packages (from pydantic<3,>=1->langchain) (0.7.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\swafey\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain) (3.4)\n",
            "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\swafey\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain) (2.0.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\swafey\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain) (1.26.14)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\swafey\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain) (2022.12.7)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\swafey\\anaconda3\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain) (2.0.1)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\swafey\\anaconda3\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.0->langchain) (2.1)\n",
            "Installing collected packages: tenacity, packaging, orjson, multidict, jsonpatch, frozenlist, async-timeout, yarl, aiosignal, langsmith, aiohttp, langchain-core, langchain-text-splitters, langchain\n",
            "  Attempting uninstall: tenacity\n",
            "    Found existing installation: tenacity 8.0.1\n",
            "    Uninstalling tenacity-8.0.1:\n",
            "      Successfully uninstalled tenacity-8.0.1\n",
            "  Attempting uninstall: packaging\n",
            "    Found existing installation: packaging 22.0\n",
            "    Uninstalling packaging-22.0:\n",
            "      Successfully uninstalled packaging-22.0\n",
            "  Attempting uninstall: jsonpatch\n",
            "    Found existing installation: jsonpatch 1.32\n",
            "    Uninstalling jsonpatch-1.32:\n",
            "      Successfully uninstalled jsonpatch-1.32\n",
            "Successfully installed aiohttp-3.9.5 aiosignal-1.3.1 async-timeout-4.0.3 frozenlist-1.4.1 jsonpatch-1.33 langchain-0.2.3 langchain-core-0.2.5 langchain-text-splitters-0.2.1 langsmith-0.1.76 multidict-6.0.5 orjson-3.10.4 packaging-23.2 tenacity-8.3.0 yarl-1.9.4\n",
            "Collecting gradient_haystack\n",
            "  Downloading gradient_haystack-0.4.0-py3-none-any.whl (11 kB)\n",
            "Collecting haystack-ai\n",
            "  Downloading haystack_ai-2.2.1-py3-none-any.whl (345 kB)\n",
            "     ------------------------------------ 345.2/345.2 kB 351.5 kB/s eta 0:00:00\n",
            "Requirement already satisfied: gradientai>=1.4.0 in c:\\users\\swafey\\anaconda3\\lib\\site-packages (from gradient_haystack) (1.13.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\swafey\\anaconda3\\lib\\site-packages (from gradientai>=1.4.0->gradient_haystack) (2.8.2)\n",
            "Requirement already satisfied: aenum>=3.1.11 in c:\\users\\swafey\\anaconda3\\lib\\site-packages (from gradientai>=1.4.0->gradient_haystack) (3.1.15)\n",
            "Requirement already satisfied: urllib3>=1.25.3 in c:\\users\\swafey\\anaconda3\\lib\\site-packages (from gradientai>=1.4.0->gradient_haystack) (1.26.14)\n",
            "Requirement already satisfied: pydantic<3,>=1.10.15 in c:\\users\\swafey\\anaconda3\\lib\\site-packages (from gradientai>=1.4.0->gradient_haystack) (2.7.3)\n",
            "Requirement already satisfied: requests in c:\\users\\swafey\\anaconda3\\lib\\site-packages (from haystack-ai->gradient_haystack) (2.28.1)\n",
            "Requirement already satisfied: pyyaml in c:\\users\\swafey\\anaconda3\\lib\\site-packages (from haystack-ai->gradient_haystack) (6.0)\n",
            "Requirement already satisfied: tqdm in c:\\users\\swafey\\anaconda3\\lib\\site-packages (from haystack-ai->gradient_haystack) (4.64.1)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in c:\\users\\swafey\\anaconda3\\lib\\site-packages (from haystack-ai->gradient_haystack) (4.12.2)\n",
            "Requirement already satisfied: tenacity in c:\\users\\swafey\\anaconda3\\lib\\site-packages (from haystack-ai->gradient_haystack) (8.3.0)\n",
            "Collecting posthog\n",
            "  Using cached posthog-3.5.0-py2.py3-none-any.whl (41 kB)\n",
            "Requirement already satisfied: numpy in c:\\users\\swafey\\anaconda3\\lib\\site-packages (from haystack-ai->gradient_haystack) (1.23.5)\n",
            "Collecting more-itertools\n",
            "  Downloading more_itertools-10.3.0-py3-none-any.whl (59 kB)\n",
            "     -------------------------------------- 59.2/59.2 kB 445.5 kB/s eta 0:00:00\n",
            "Requirement already satisfied: pandas in c:\\users\\swafey\\anaconda3\\lib\\site-packages (from haystack-ai->gradient_haystack) (1.5.3)\n",
            "Collecting lazy-imports\n",
            "  Using cached lazy_imports-0.3.1-py3-none-any.whl (12 kB)\n",
            "Requirement already satisfied: networkx in c:\\users\\swafey\\anaconda3\\lib\\site-packages (from haystack-ai->gradient_haystack) (2.8.4)\n",
            "Collecting openai>=1.1.0\n",
            "  Downloading openai-1.33.0-py3-none-any.whl (325 kB)\n",
            "     ------------------------------------ 325.5/325.5 kB 469.1 kB/s eta 0:00:00\n",
            "Requirement already satisfied: jinja2 in c:\\users\\swafey\\anaconda3\\lib\\site-packages (from haystack-ai->gradient_haystack) (3.1.2)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\swafey\\anaconda3\\lib\\site-packages (from openai>=1.1.0->haystack-ai->gradient_haystack) (3.5.0)\n",
            "Requirement already satisfied: sniffio in c:\\users\\swafey\\anaconda3\\lib\\site-packages (from openai>=1.1.0->haystack-ai->gradient_haystack) (1.2.0)\n",
            "Collecting distro<2,>=1.7.0\n",
            "  Using cached distro-1.9.0-py3-none-any.whl (20 kB)\n",
            "Collecting httpx<1,>=0.23.0\n",
            "  Using cached httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\swafey\\anaconda3\\lib\\site-packages (from pydantic<3,>=1.10.15->gradientai>=1.4.0->gradient_haystack) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.4 in c:\\users\\swafey\\anaconda3\\lib\\site-packages (from pydantic<3,>=1.10.15->gradientai>=1.4.0->gradient_haystack) (2.18.4)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\swafey\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->gradientai>=1.4.0->gradient_haystack) (1.16.0)\n",
            "Requirement already satisfied: colorama in c:\\users\\swafey\\anaconda3\\lib\\site-packages (from tqdm->haystack-ai->gradient_haystack) (0.4.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\swafey\\anaconda3\\lib\\site-packages (from jinja2->haystack-ai->gradient_haystack) (2.1.1)\n",
            "Requirement already satisfied: pytz>=2020.1 in c:\\users\\swafey\\anaconda3\\lib\\site-packages (from pandas->haystack-ai->gradient_haystack) (2022.7)\n",
            "Collecting monotonic>=1.5\n",
            "  Using cached monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n",
            "Collecting backoff>=1.10.0\n",
            "  Using cached backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\swafey\\anaconda3\\lib\\site-packages (from requests->haystack-ai->gradient_haystack) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\swafey\\anaconda3\\lib\\site-packages (from requests->haystack-ai->gradient_haystack) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\swafey\\anaconda3\\lib\\site-packages (from requests->haystack-ai->gradient_haystack) (3.4)\n",
            "Collecting httpcore==1.*\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "     -------------------------------------- 77.9/77.9 kB 541.5 kB/s eta 0:00:00\n",
            "Collecting h11<0.15,>=0.13\n",
            "  Using cached h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "Installing collected packages: monotonic, more-itertools, lazy-imports, h11, distro, backoff, posthog, httpcore, httpx, openai, haystack-ai, gradient_haystack\n",
            "Successfully installed backoff-2.2.1 distro-1.9.0 gradient_haystack-0.4.0 h11-0.14.0 haystack-ai-2.2.1 httpcore-1.0.5 httpx-0.27.0 lazy-imports-0.3.1 monotonic-1.6 more-itertools-10.3.0 openai-1.33.0 posthog-3.5.0\n",
            "Requirement already satisfied: regex in c:\\users\\swafey\\anaconda3\\lib\\site-packages (2022.7.9)\n",
            "Collecting rouge\n",
            "  Downloading rouge-1.0.1-py3-none-any.whl (13 kB)\n",
            "Requirement already satisfied: six in c:\\users\\swafey\\anaconda3\\lib\\site-packages (from rouge) (1.16.0)\n",
            "Installing collected packages: rouge\n",
            "Successfully installed rouge-1.0.1\n",
            "Requirement already satisfied: nltk in c:\\users\\swafey\\anaconda3\\lib\\site-packages (3.7)\n",
            "Requirement already satisfied: joblib in c:\\users\\swafey\\anaconda3\\lib\\site-packages (from nltk) (1.1.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\swafey\\anaconda3\\lib\\site-packages (from nltk) (2022.7.9)\n",
            "Requirement already satisfied: tqdm in c:\\users\\swafey\\anaconda3\\lib\\site-packages (from nltk) (4.64.1)\n",
            "Requirement already satisfied: click in c:\\users\\swafey\\anaconda3\\lib\\site-packages (from nltk) (8.0.4)\n",
            "Requirement already satisfied: colorama in c:\\users\\swafey\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.6)\n",
            "Collecting wandb\n",
            "  Downloading wandb-0.17.1-py3-none-win_amd64.whl (6.7 MB)\n",
            "     ---------------------------------------- 6.7/6.7 MB 274.3 kB/s eta 0:00:00\n",
            "Collecting docker-pycreds>=0.4.0\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Collecting sentry-sdk>=1.0.0\n",
            "  Downloading sentry_sdk-2.5.1-py2.py3-none-any.whl (289 kB)\n",
            "     ------------------------------------ 289.6/289.6 kB 303.0 kB/s eta 0:00:00\n",
            "Collecting setproctitle\n",
            "  Downloading setproctitle-1.3.3-cp310-cp310-win_amd64.whl (11 kB)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in c:\\users\\swafey\\anaconda3\\lib\\site-packages (from wandb) (8.0.4)\n",
            "Collecting gitpython!=3.1.29,>=1.0.0\n",
            "  Downloading GitPython-3.1.43-py3-none-any.whl (207 kB)\n",
            "     ------------------------------------ 207.3/207.3 kB 394.2 kB/s eta 0:00:00\n",
            "Requirement already satisfied: psutil>=5.0.0 in c:\\users\\swafey\\anaconda3\\lib\\site-packages (from wandb) (5.9.0)\n",
            "Requirement already satisfied: platformdirs in c:\\users\\swafey\\anaconda3\\lib\\site-packages (from wandb) (2.5.2)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in c:\\users\\swafey\\anaconda3\\lib\\site-packages (from wandb) (2.28.1)\n",
            "Collecting protobuf!=4.21.0,<6,>=3.19.0\n",
            "  Downloading protobuf-5.27.1-cp310-abi3-win_amd64.whl (426 kB)\n",
            "     ------------------------------------ 426.9/426.9 kB 365.5 kB/s eta 0:00:00\n",
            "Requirement already satisfied: pyyaml in c:\\users\\swafey\\anaconda3\\lib\\site-packages (from wandb) (6.0)\n",
            "Requirement already satisfied: setuptools in c:\\users\\swafey\\anaconda3\\lib\\site-packages (from wandb) (65.6.3)\n",
            "Requirement already satisfied: colorama in c:\\users\\swafey\\anaconda3\\lib\\site-packages (from click!=8.0.0,>=7.1->wandb) (0.4.6)\n",
            "Requirement already satisfied: six>=1.4.0 in c:\\users\\swafey\\anaconda3\\lib\\site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "  Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
            "     -------------------------------------- 62.7/62.7 kB 374.9 kB/s eta 0:00:00\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\swafey\\anaconda3\\lib\\site-packages (from requests<3,>=2.0.0->wandb) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\swafey\\anaconda3\\lib\\site-packages (from requests<3,>=2.0.0->wandb) (1.26.14)\n",
            "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\swafey\\anaconda3\\lib\\site-packages (from requests<3,>=2.0.0->wandb) (2.0.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\swafey\\anaconda3\\lib\\site-packages (from requests<3,>=2.0.0->wandb) (3.4)\n",
            "Collecting smmap<6,>=3.0.1\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: smmap, setproctitle, sentry-sdk, protobuf, docker-pycreds, gitdb, gitpython, wandb\n",
            "Successfully installed docker-pycreds-0.4.0 gitdb-4.0.11 gitpython-3.1.43 protobuf-5.27.1 sentry-sdk-2.5.1 setproctitle-1.3.3 smmap-5.0.1 wandb-0.17.1\n",
            "Collecting datasets\n",
            "  Downloading datasets-2.19.2-py3-none-any.whl (542 kB)\n",
            "     ------------------------------------ 542.1/542.1 kB 206.3 kB/s eta 0:00:00\n",
            "Collecting requests>=2.32.1\n",
            "  Downloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
            "     --------------------------------------- 64.9/64.9 kB 68.6 kB/s eta 0:00:00\n",
            "Requirement already satisfied: filelock in c:\\users\\swafey\\anaconda3\\lib\\site-packages (from datasets) (3.9.0)\n",
            "Collecting xxhash\n",
            "  Downloading xxhash-3.4.1-cp310-cp310-win_amd64.whl (29 kB)\n",
            "Collecting huggingface-hub>=0.21.2\n",
            "  Downloading huggingface_hub-0.23.3-py3-none-any.whl (401 kB)\n",
            "     ------------------------------------ 401.7/401.7 kB 191.2 kB/s eta 0:00:00\n",
            "Requirement already satisfied: packaging in c:\\users\\swafey\\anaconda3\\lib\\site-packages (from datasets) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\swafey\\anaconda3\\lib\\site-packages (from datasets) (6.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\swafey\\anaconda3\\lib\\site-packages (from datasets) (0.3.6)\n",
            "Collecting fsspec[http]<=2024.3.1,>=2023.1.0\n",
            "  Downloading fsspec-2024.3.1-py3-none-any.whl (171 kB)\n",
            "     ------------------------------------ 172.0/172.0 kB 178.3 kB/s eta 0:00:00\n",
            "Requirement already satisfied: numpy>=1.17 in c:\\users\\swafey\\anaconda3\\lib\\site-packages (from datasets) (1.23.5)\n",
            "Requirement already satisfied: pandas in c:\\users\\swafey\\anaconda3\\lib\\site-packages (from datasets) (1.5.3)\n",
            "Collecting multiprocess\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "     ------------------------------------ 134.8/134.8 kB 169.7 kB/s eta 0:00:00\n",
            "Requirement already satisfied: aiohttp in c:\\users\\swafey\\anaconda3\\lib\\site-packages (from datasets) (3.9.5)\n",
            "Collecting pyarrow-hotfix\n",
            "  Downloading pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in c:\\users\\swafey\\anaconda3\\lib\\site-packages (from datasets) (4.64.1)\n",
            "Collecting pyarrow>=12.0.0\n",
            "  Downloading pyarrow-16.1.0-cp310-cp310-win_amd64.whl (25.9 MB)\n",
            "     -------------------------------------- 25.9/25.9 MB 448.0 kB/s eta 0:00:00\n",
            "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\swafey\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (22.1.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\swafey\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in c:\\users\\swafey\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\swafey\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\swafey\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\swafey\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\swafey\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.21.2->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\swafey\\anaconda3\\lib\\site-packages (from requests>=2.32.1->datasets) (2.0.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\swafey\\anaconda3\\lib\\site-packages (from requests>=2.32.1->datasets) (1.26.14)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\swafey\\anaconda3\\lib\\site-packages (from requests>=2.32.1->datasets) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\swafey\\anaconda3\\lib\\site-packages (from requests>=2.32.1->datasets) (3.4)\n",
            "Requirement already satisfied: colorama in c:\\users\\swafey\\anaconda3\\lib\\site-packages (from tqdm>=4.62.1->datasets) (0.4.6)\n",
            "Collecting dill<0.3.9,>=0.3.0\n",
            "  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "     ------------------------------------ 116.3/116.3 kB 751.9 kB/s eta 0:00:00\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\swafey\\anaconda3\\lib\\site-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in c:\\users\\swafey\\anaconda3\\lib\\site-packages (from pandas->datasets) (2022.7)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\swafey\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
            "Installing collected packages: xxhash, requests, pyarrow-hotfix, pyarrow, fsspec, dill, multiprocess, huggingface-hub, datasets\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.28.1\n",
            "    Uninstalling requests-2.28.1:\n",
            "      Successfully uninstalled requests-2.28.1\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2022.11.0\n",
            "    Uninstalling fsspec-2022.11.0:\n",
            "      Successfully uninstalled fsspec-2022.11.0\n",
            "  Attempting uninstall: dill\n",
            "    Found existing installation: dill 0.3.6\n",
            "    Uninstalling dill-0.3.6:\n",
            "      Successfully uninstalled dill-0.3.6\n",
            "  Attempting uninstall: huggingface-hub\n",
            "    Found existing installation: huggingface-hub 0.10.1\n",
            "    Uninstalling huggingface-hub-0.10.1:\n",
            "      Successfully uninstalled huggingface-hub-0.10.1\n",
            "Successfully installed datasets-2.19.2 dill-0.3.8 fsspec-2024.3.1 huggingface-hub-0.23.3 multiprocess-0.70.16 pyarrow-16.1.0 pyarrow-hotfix-0.6 requests-2.32.3 xxhash-3.4.1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "conda-repo-cli 1.0.41 requires requests_mock, which is not installed.\n",
            "conda-repo-cli 1.0.41 requires clyent==1.2.1, but you have clyent 1.2.2 which is incompatible.\n",
            "conda-repo-cli 1.0.41 requires nbformat==5.4.0, but you have nbformat 5.7.0 which is incompatible.\n",
            "conda-repo-cli 1.0.41 requires requests==2.28.1, but you have requests 2.32.3 which is incompatible.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: rouge in c:\\users\\swafey\\anaconda3\\lib\\site-packages (1.0.1)\n",
            "Requirement already satisfied: six in c:\\users\\swafey\\anaconda3\\lib\\site-packages (from rouge) (1.16.0)\n",
            "Requirement already satisfied: nltk in c:\\users\\swafey\\anaconda3\\lib\\site-packages (3.7)\n",
            "Requirement already satisfied: joblib in c:\\users\\swafey\\anaconda3\\lib\\site-packages (from nltk) (1.1.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\swafey\\anaconda3\\lib\\site-packages (from nltk) (2022.7.9)\n",
            "Requirement already satisfied: tqdm in c:\\users\\swafey\\anaconda3\\lib\\site-packages (from nltk) (4.64.1)\n",
            "Requirement already satisfied: click in c:\\users\\swafey\\anaconda3\\lib\\site-packages (from nltk) (8.0.4)\n",
            "Requirement already satisfied: colorama in c:\\users\\swafey\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.6)\n",
            "Collecting langchain_community\n",
            "  Downloading langchain_community-0.2.4-py3-none-any.whl (2.2 MB)\n",
            "     ---------------------------------------- 2.2/2.2 MB 351.9 kB/s eta 0:00:00\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\swafey\\anaconda3\\lib\\site-packages (from langchain_community) (1.4.39)\n",
            "Requirement already satisfied: numpy<2,>=1 in c:\\users\\swafey\\anaconda3\\lib\\site-packages (from langchain_community) (1.23.5)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\swafey\\anaconda3\\lib\\site-packages (from langchain_community) (3.9.5)\n",
            "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\swafey\\anaconda3\\lib\\site-packages (from langchain_community) (6.0)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.0 in c:\\users\\swafey\\anaconda3\\lib\\site-packages (from langchain_community) (0.1.76)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in c:\\users\\swafey\\anaconda3\\lib\\site-packages (from langchain_community) (8.3.0)\n",
            "Requirement already satisfied: requests<3,>=2 in c:\\users\\swafey\\anaconda3\\lib\\site-packages (from langchain_community) (2.32.3)\n",
            "Requirement already satisfied: langchain-core<0.3.0,>=0.2.0 in c:\\users\\swafey\\anaconda3\\lib\\site-packages (from langchain_community) (0.2.5)\n",
            "Requirement already satisfied: langchain<0.3.0,>=0.2.0 in c:\\users\\swafey\\anaconda3\\lib\\site-packages (from langchain_community) (0.2.3)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\swafey\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\swafey\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in c:\\users\\swafey\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (4.0.3)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\swafey\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.4.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\swafey\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (22.1.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\swafey\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.3.1)\n",
            "Collecting typing-inspect<1,>=0.4.0\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0\n",
            "  Downloading marshmallow-3.21.3-py3-none-any.whl (49 kB)\n",
            "     -------------------------------------- 49.2/49.2 kB 628.4 kB/s eta 0:00:00\n",
            "Requirement already satisfied: pydantic<3,>=1 in c:\\users\\swafey\\anaconda3\\lib\\site-packages (from langchain<0.3.0,>=0.2.0->langchain_community) (2.7.3)\n",
            "Requirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in c:\\users\\swafey\\anaconda3\\lib\\site-packages (from langchain<0.3.0,>=0.2.0->langchain_community) (0.2.1)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\swafey\\anaconda3\\lib\\site-packages (from langchain-core<0.3.0,>=0.2.0->langchain_community) (1.33)\n",
            "Requirement already satisfied: packaging<24.0,>=23.2 in c:\\users\\swafey\\anaconda3\\lib\\site-packages (from langchain-core<0.3.0,>=0.2.0->langchain_community) (23.2)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\swafey\\anaconda3\\lib\\site-packages (from langsmith<0.2.0,>=0.1.0->langchain_community) (3.10.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\swafey\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain_community) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\swafey\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain_community) (2.0.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\swafey\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain_community) (1.26.14)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\swafey\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain_community) (3.4)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\swafey\\anaconda3\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain_community) (2.0.1)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\swafey\\anaconda3\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.0->langchain_community) (2.1)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\swafey\\anaconda3\\lib\\site-packages (from pydantic<3,>=1->langchain<0.3.0,>=0.2.0->langchain_community) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.4 in c:\\users\\swafey\\anaconda3\\lib\\site-packages (from pydantic<3,>=1->langchain<0.3.0,>=0.2.0->langchain_community) (2.18.4)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in c:\\users\\swafey\\anaconda3\\lib\\site-packages (from pydantic<3,>=1->langchain<0.3.0,>=0.2.0->langchain_community) (4.12.2)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\swafey\\anaconda3\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community) (0.4.3)\n",
            "Installing collected packages: typing-inspect, marshmallow, dataclasses-json, langchain_community\n",
            "Successfully installed dataclasses-json-0.6.7 langchain_community-0.2.4 marshmallow-3.21.3 typing-inspect-0.9.0\n"
          ]
        }
      ],
      "source": [
        "!pip install gradientai --upgrade\n",
        "!pip install langchain\n",
        "!pip install -U gradient_haystack\n",
        "!pip install regex\n",
        "!pip install rouge\n",
        "!pip install nltk\n",
        "!pip install wandb\n",
        "!pip install datasets\n",
        "!pip install rouge\n",
        "!pip install nltk\n",
        "!pip install langchain_community\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ids19xUiPP_A"
      },
      "source": [
        "## **1.Loading the Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OWjkVgavPkjM",
        "outputId": "2e21bb03-cfb0-4d08-a8d8-acffb200e4a6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset Size: 1412\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import requests\n",
        "\n",
        "# URL of the online repository where the dataset is hosted\n",
        "dataset_url = \"https://raw.githubusercontent.com/swafey-karanja/Model-training/main/NewData.json\"\n",
        "\n",
        "# Make an HTTP GET request to fetch the dataset\n",
        "response = requests.get(dataset_url)\n",
        "\n",
        "# Check if the request was successful (status code 200)\n",
        "if response.status_code == 200:\n",
        "    # Load the dataset from the response content\n",
        "    train_dataset = json.loads(response.text)\n",
        "\n",
        "    # Print the size of the dataset\n",
        "    print(\"Dataset Size:\", len(train_dataset))\n",
        "else:\n",
        "    # Print an error message if the request failed\n",
        "    print(\"Failed to fetch dataset. Status code:\", response.status_code)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KnzGaOMVpGf4"
      },
      "source": [
        "**Break the data into batches**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DYzINIs6pErW",
        "outputId": "8d345e11-1749-4471-d2ec-12dc23dfb03e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batches size\n",
            "[100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 12]\n"
          ]
        }
      ],
      "source": [
        "def divide_into_Batches(number, chunk_size):  # Define a function to divide a number into chunks of a given size\n",
        "    Batches = []\n",
        "    while number > 0:\n",
        "        if number >= chunk_size:\n",
        "            Batches.append(chunk_size)\n",
        "            number -= chunk_size\n",
        "        else:\n",
        "            Batches.append(number)\n",
        "            break\n",
        "\n",
        "    return Batches\n",
        "\n",
        "Batches = divide_into_Batches(len(train_dataset), 100)  # Divide the dataset into chunks of 100 samples each\n",
        "print(\"Batches size\")\n",
        "print(Batches)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "apmpbpeppW2T"
      },
      "source": [
        "## **2.Load the Base Model.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I-W4W_s3pySl"
      },
      "source": [
        "Loading the Nous-Hermes-Llama2-13b base model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "uPy0iC61pj1o"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ['GRADIENT_ACCESS_TOKEN'] = \"4RkXwcXCIhjSilcrkYNanvSI8h1WWrgt\"\n",
        "os.environ['GRADIENT_WORKSPACE_ID'] = \"496b8f01-47f9-4f62-91c8-e634679ca2d3_workspace\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UOarEjaEpvWX",
        "outputId": "39907253-f18d-4b1d-8a53-c7c87e9d36cd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Available Base Models\n",
            "\t <gradientai._base_model.BaseModel object at 0x0000020F44497250>\n",
            "\t <gradientai._base_model.BaseModel object at 0x0000020F444961D0>\n",
            "\t <gradientai._base_model.BaseModel object at 0x0000020F444973A0>\n",
            "\t <gradientai._base_model.BaseModel object at 0x0000020F44496B60>\n",
            "\t <gradientai._base_model.BaseModel object at 0x0000020F444966E0>\n",
            "\t <gradientai._base_model.BaseModel object at 0x0000020F444965C0>\n",
            "\t <gradientai._base_model.BaseModel object at 0x0000020F444969E0>\n",
            "\n",
            "Base Model Chosen : <gradientai._base_model.BaseModel object at 0x0000020F44497550>\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import os\n",
        "from gradientai import Gradient\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.llms import GradientLLM\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "\n",
        "os.environ['GRADIENT_ACCESS_TOKEN'] = \"4RkXwcXCIhjSilcrkYNanvSI8h1WWrgt\"\n",
        "os.environ['GRADIENT_WORKSPACE_ID'] = \"496b8f01-47f9-4f62-91c8-e634679ca2d3_workspace\"\n",
        "\n",
        "gradient =  Gradient()\n",
        "\n",
        "print(\"Available Base Models\")\n",
        "for i in gradient.list_models(only_base=True):\n",
        "    print(\"\\t\",i)\n",
        "\n",
        "base_model_id = \"NousResearch/Nous-Hermes-Llama2-13b\"\n",
        "base_model_name = \"nous-hermes2\"\n",
        "base_model = gradient.get_base_model(base_model_slug=\"nous-hermes2\") # base model Nous-Hermes-Llama2-13b\n",
        "\n",
        "print(\"\\nBase Model Chosen :\", base_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G2WHiZMvtxGg"
      },
      "source": [
        "##**3. Creating a Model Adapter**\n",
        "\n",
        "* Adapters are small, lightweight modules inserted between\n",
        "existing layers of a pre-trained LLM. They act like \"add-ons\" that focus on learning task-specific information without modifying the core knowledge captured in the original model.\n",
        "\n",
        "* The addapter server as the object that we are going to fine tune"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zKK8fwv5t_mX",
        "outputId": "45e7be51-95c1-4ca7-a213-f035b8b75510"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Base model id                : cc2dafce-9e6e-4a23-a918-cad6ba89e42e_base_ml_model\n",
            "Fine tune model Name         : Llama2-13b/Vet-assistant-Newdata\n",
            "Fine tune model adapter id   : 2e7a849b-239e-44a1-a88a-3be1d77c1f93_model_adapter\n",
            "\n",
            "\n",
            "\n",
            "Size of object in memory, in bytes. 56\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'_api_instance': <gradientai.openapi.client.api.models_api.ModelsApi at 0x7b7224a41630>,\n",
              " '_id': '2e7a849b-239e-44a1-a88a-3be1d77c1f93_model_adapter',\n",
              " '_workspace_id': 'c571b959-4ce8-474a-b3f3-398c7b347c57_workspace',\n",
              " '_async_semaphore': <asyncio.locks.Semaphore object at 0x7b7224a42680 [unlocked, value:8]>,\n",
              " '_base_model_id': 'cc2dafce-9e6e-4a23-a918-cad6ba89e42e_base_ml_model',\n",
              " '_name': 'Llama2-13b/Vet-assistant-Newdata'}"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# our_finetune_model_name=\"Llama2-13b/Vet-assistant-Newdata\"\n",
        "# Fine_Tune__adapter = base_model.create_model_adapter(\n",
        "#         name=our_finetune_model_name, # base mode nous hermis\n",
        "#         learning_rate=0.00005, #  Determines how fast a model updates its knowledge during fine-tuning.\n",
        "#         rank=8,  # Dimensionality\n",
        "\n",
        "#     )\n",
        "\n",
        "# # default hyperparameters Frozen\n",
        "# hyperparameters = {\n",
        "#                   \"block_size\": 1024,\n",
        "#                   \"model_max_length\": 2048,\n",
        "#                   \"padding\": \"right\",\n",
        "#                   \"use_flash_attention_2\": False,\n",
        "#                   \"disable_gradient_checkpointing\": False,\n",
        "#                   \"logging_steps\": -1,\n",
        "#                   \"evaluation_strategy\": \"epoch\",\n",
        "#                   \"save_total_limit\": 1,\n",
        "#                   \"save_strategy\": \"epoch\",\n",
        "#                   \"auto_find_batch_size\": False,\n",
        "#                   \"mixed_precision\": \"fp16\",\n",
        "#                   \"epochs\": 3,\n",
        "#                   \"batch_size\": 100,\n",
        "#                   \"warmup_ratio\": 0.1,\n",
        "#                   \"gradient_accumulation\": 1,\n",
        "#                   \"optimizer\": \"adamw_torch\",\n",
        "#                   \"scheduler\": \"linear\",\n",
        "#                   \"weight_decay\": 0,\n",
        "#                   \"max_grad_norm\": 1,\n",
        "#                   \"seed\": 42,\n",
        "#                   \"apply_chat_template\": False,\n",
        "#                   \"quantization\": \"int4\",\n",
        "#                   \"target_modules\": \"\",\n",
        "#                   \"merge_adapter\": False,\n",
        "#                   \"peft\": True,\n",
        "#                   \"lora_r\": 16,\n",
        "#                   \"lora_alpha\": 32,\n",
        "#                   \"lora_dropout\": 0.05\n",
        "#   }\n",
        "\n",
        "\n",
        "# print(f\"Base model id                : {Fine_Tune__adapter._base_model_id}\")\n",
        "# print(f\"Fine tune model Name         : { Fine_Tune__adapter.name}\")\n",
        "# print(f\"Fine tune model adapter id   : {Fine_Tune__adapter.id}\")\n",
        "\n",
        "# print(\"\\n\\n\")\n",
        "# print(\"Size of object in memory, in bytes.\", Fine_Tune__adapter.__format__.__sizeof__()) # Size of object in memory, in bytes.\n",
        "# Fine_Tune__adapter.__dict__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fPU-mOK0uzfF"
      },
      "source": [
        "## **4. Fine Tuning The Adaptor**\n",
        "\n",
        "For this case we will be performing Laura-based finetuning which means that we are freezing about 99% of the layers and then finetuning an adapter on top of it.\n",
        "\n",
        "LoRA: Low-Rank Adaptation of Large Language Models is a novel technique introduced by Microsoft researchers to deal with the problem of fine-tuning large-language models with billions of parameters\n",
        "\n",
        " LoRA proposes to freeze pre-trained model weights and inject trainable layers (rank-decomposition matrices) in each transformer block. This greatly reduces the number of trainable parameters and GPU memory requirements since gradients don't need to be computed for most model weights\n",
        "\n",
        "\n",
        "why LoRA finetuning:\n",
        "\n",
        "1. Faster Training: Since only the added task-specific layers are trained while the pre-trained model's parameters remain frozen, the fine-tuning process is generally faster compared to training a model from scratch\n",
        "2. Computation requirements are lower. We could create a full fine-tuned model in a 2080 Ti with 11 GB of VRAM!\n",
        "3. Trained weights are  much smaller. Because the original model is frozen and we inject new layers to be trained\n",
        "\n",
        " [for more info](https://huggingface.co/blog/lora)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JhFoAaE5vPAk",
        "outputId": "93a47e79-6536-4ea5-e1f1-312e453ca2eb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Our Model id:  2e7a849b-239e-44a1-a88a-3be1d77c1f93_model_adapter\n",
            "================================================================\n",
            "\n",
            "Fine tuning . . .\n",
            "\n",
            "Fine-tuning the model, iteration 1\n",
            "Batch 1 range: 0 : 100\n",
            "\t Batch 1 Evaluation : number_of_trainable_tokens=8007 sum_loss=18044.285\n",
            "Batch 2 range: 100 : 200\n",
            "\t Batch 2 Evaluation : number_of_trainable_tokens=8043 sum_loss=11213.263\n",
            "Batch 3 range: 200 : 300\n",
            "\t Batch 3 Evaluation : number_of_trainable_tokens=9278 sum_loss=10536.057\n",
            "Batch 4 range: 300 : 400\n",
            "\t Batch 4 Evaluation : number_of_trainable_tokens=10454 sum_loss=11967.004\n",
            "Batch 5 range: 400 : 500\n",
            "\t Batch 5 Evaluation : number_of_trainable_tokens=13087 sum_loss=13200.826\n",
            "Batch 6 range: 500 : 600\n",
            "\t Batch 6 Evaluation : number_of_trainable_tokens=16332 sum_loss=17247.604\n",
            "Batch 7 range: 600 : 700\n",
            "\t Batch 7 Evaluation : number_of_trainable_tokens=18421 sum_loss=17550.602\n",
            "Batch 8 range: 700 : 800\n",
            "\t Batch 8 Evaluation : number_of_trainable_tokens=18931 sum_loss=17459.293\n",
            "Batch 9 range: 800 : 900\n",
            "\t Batch 9 Evaluation : number_of_trainable_tokens=13457 sum_loss=11873.373\n",
            "Batch 10 range: 900 : 1000\n",
            "\t Batch 10 Evaluation : number_of_trainable_tokens=10560 sum_loss=9307.085\n",
            "Batch 11 range: 1000 : 1100\n",
            "\t Batch 11 Evaluation : number_of_trainable_tokens=9856 sum_loss=6688.8364\n",
            "Batch 12 range: 1100 : 1200\n",
            "\t Batch 12 Evaluation : number_of_trainable_tokens=8558 sum_loss=6130.7583\n",
            "Batch 13 range: 1200 : 1300\n",
            "\t Batch 13 Evaluation : number_of_trainable_tokens=5507 sum_loss=5665.937\n",
            "Batch 14 range: 1300 : 1400\n",
            "\t Batch 14 Evaluation : number_of_trainable_tokens=5389 sum_loss=5073.09\n",
            "Batch 15 range: 1400 : 1412\n",
            "\t Batch 15 Evaluation : number_of_trainable_tokens=999 sum_loss=954.4346\n"
          ]
        }
      ],
      "source": [
        "# print(f\"Our Model id:  {Fine_Tune__adapter.id}\")\n",
        "# num_epochs = 1  # num_epochs is the number of times you fine-tune the model # more epochs tends to get better results, but you also run the risk of \"overfitting\"\n",
        "# count = 0\n",
        "# print(\"================================================================\\n\")\n",
        "# print(\"Fine tuning . . .\\n\")\n",
        "# while count < num_epochs:\n",
        "#     print(f\"Fine-tuning the model, iteration {count + 1}\")\n",
        "#     s = 0\n",
        "#     n = 1\n",
        "#     for Batch in Batches:\n",
        "#         print(f\"Batch {n} range: {s} : {(s + Batch)}\")\n",
        "\n",
        "#         # Try to fine-tune the model with the chunk of samples,\n",
        "#         while True:\n",
        "#             try:\n",
        "#                 metric = Fine_Tune__adapter.fine_tune(samples=train_dataset[s: s + Batch])\n",
        "#                 print(f\"\\t Batch {n} Evaluation :\", metric)\n",
        "#                 break\n",
        "#             except:\n",
        "#                 pass\n",
        "\n",
        "\n",
        "\n",
        "#         s += Batch\n",
        "#         n += 1\n",
        "#     count = count + 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JH8ghczL_Xy6"
      },
      "source": [
        "## **5.Model Inference**\n",
        "\n",
        "\"model inference\" typically refers to the process of using a trained model to make predictions on new, unseen data. Fine-tuning a model involves taking a pre-trained model and further training it on a specific task or dataset to improve its performance.\n",
        "\n",
        "When fine-tuning a model, the process of inference remains the same as with any other trained model. Once the fine-tuning is complete, you can use the model to make predictions on new data by passing the data through the model and obtaining the output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "fsGJw4hw_8O5"
      },
      "outputs": [],
      "source": [
        "from langchain.chains import LLMChain    # Import the LLMChain class for building LLM-based workflows\n",
        "from langchain.llms import GradientLLM   # Import the GradientLLM class for interacting with Gradient AI's API\n",
        "from langchain.prompts import PromptTemplate # Import the PromptTemplate class for defining how to prompt the LLM\n",
        "import gradientai\n",
        "import os # Import the os module for potential file system interactions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "3Vbg5iMnAA1-"
      },
      "outputs": [],
      "source": [
        "Fine_Tune__adapter_ID = \"28643f93-bdd5-4602-b911-2e9fea183186_model_adapter\"\n",
        "#Fine_Tune__adapter_ID = Fine_Tune__adapter.id\n",
        "#  creating a GradientLLM object\n",
        "llm = GradientLLM(\n",
        "    model=Fine_Tune__adapter_ID,\n",
        "    model_kwargs=dict(\n",
        "        max_generated_token_count=128, # Adjust how your model generates completions\n",
        "        temperature = 0.7, # randomness\n",
        "        top_k=50 # Restricts the model to pick from k most likely words,\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vVCG5MR7Au16"
      },
      "source": [
        "### **Formatting Prompts**\n",
        "\n",
        "The model follows the Alpaca prompt format which provides a structured way to input information to the model, guiding it on what kind of output is desired."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fzevoH6vBLXc",
        "outputId": "0e0e7aae-62cf-4fac-8749-e199262db22f"
      },
      "outputs": [],
      "source": [
        "template = \"\"\"### Instruction: {Instruction} \\n\\n### Response:\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(template=template, input_variables=[\"Instruction\"])\n",
        "\n",
        "llm_chain = LLMChain(prompt=prompt, llm=llm)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vg0GwPNHBmjo"
      },
      "source": [
        "#### *Example* *inputs* *and* *corresponding* *outputs*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hcTdHlRZBcN7",
        "outputId": "795f11ac-b395-47d5-99b8-47043f17d66f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Question :\n",
            " How does Clostridial Diseases impact milk production, reproductive performance, and overall well-being in cows?\n",
            "Answer :\n",
            " Clostridial Diseases impact milk production, reproductive performance, and overall well-being in cows. \n",
            "\n",
            "\n",
            "Question :\n",
            " what are some of the risk factors associated with lameness in dairy cows?\n",
            "Answer :\n",
            " Risk factors associated with lameness in dairy cows include: hoof imbalance, laminitis, foot abscesses and trauma. \n",
            "\n",
            "\n",
            "Question :\n",
            " How dose lameness impact mill production, reproduction and overall well-being in cows\n",
            "Answer :\n",
            " Lameness in cattle can be debilitating and impact production, reproduction, and overall well-being. \n",
            "\n",
            "\n",
            "Question :\n",
            " What diseases are prevelant in dairy small ruminant, and what managment practice can mitigate their impact \n",
            "Answer :\n",
            " Diseases prevalent in dairy small ruminant include mastitis, foot rot, and metritis. Management practices such as proper sanitation, hoof trimming, and vaccination can mitigate their impact. \n",
            "\n",
            "\n",
            "Question :\n",
            " what specific health managment strategies should be implemented to prevent or treat common cow diseases?\n",
            "Answer :\n",
            " Implementing specific health management strategies, such as routine vaccination, fly control, and foot care, can prevent or treat common cow diseases. \n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "Question1 = \"How does Clostridial Diseases impact milk production, reproductive performance, and overall well-being in cows?\"\n",
        "Question2 = \"what are some of the risk factors associated with lameness in dairy cows?\"\n",
        "Question3 = \"How dose lameness impact mill production, reproduction and overall well-being in cows\"\n",
        "Question4 = \"What diseases are prevelant in dairy small ruminant, and what managment practice can mitigate their impact \"\n",
        "Question5 = \"what specific health managment strategies should be implemented to prevent or treat common cow diseases?\"\n",
        "\n",
        "\n",
        "print(\"Question :\\n\", Question1)\n",
        "Answer = llm_chain.run(Instruction=f\"{Question1}\")\n",
        "print(\"Answer :\\n\", Answer, \"\\n\\n\")\n",
        "\n",
        "print(\"Question :\\n\", Question2)\n",
        "Answer = llm_chain.run(Instruction=f\"{Question2}\")\n",
        "print(\"Answer :\\n\", Answer, \"\\n\\n\")\n",
        "\n",
        "print(\"Question :\\n\", Question3)\n",
        "Answer = llm_chain.run(Instruction=f\"{Question3}\")\n",
        "print(\"Answer :\\n\", Answer, \"\\n\\n\")\n",
        "\n",
        "print(\"Question :\\n\", Question4)\n",
        "Answer = llm_chain.run(Instruction=f\"{Question4}\")\n",
        "print(\"Answer :\\n\", Answer, \"\\n\\n\")\n",
        "\n",
        "print(\"Question :\\n\", Question5)\n",
        "Answer = llm_chain.run(Instruction=f\"{Question5}\")\n",
        "print(\"Answer :\\n\", Answer, \"\\n\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sCh6YPY-DPh8"
      },
      "source": [
        "## **6.Model Evaluation**\n",
        "\n",
        "Here we are using two popular automatic evaluation metrics to assess the performance of your LLM:\n",
        "\n",
        "* BLEU score: This metric calculates the n-gram precision between the generated response and the reference response\n",
        "\n",
        "* ROUGE score: This metric measures the overlap in word n-grams and longest common subsequences between the generated response and the reference response.\n",
        "\n",
        "BLEU and ROUGE scores are calculated to compare the generated response with the target response."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "MxVdOqQJEZNS"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import json\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "from rouge import Rouge\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.llms import GradientLLM\n",
        "from langchain.prompts import PromptTemplate\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sHfeZ8FfEbXp",
        "outputId": "8dc2733d-9504-4d06-9e79-9a37b2c07044"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " =================================== Evaluation =================================== \n",
            "\n",
            " ---------------------------------------------------------------\n",
            "INPUT QUERY:\n",
            " How does technology contribute to advancements in animal husbandry?\n",
            "\n",
            "TARGET RESPONSE:\n",
            " Technology in animal husbandry includes innovations like automated feeding systems, precision breeding techniques, and health monitoring devices. These advancements enhance efficiency, reduce costs, and improve overall management practices.\n",
            "\n",
            "LLM RESPONSE:\n",
            " Technology contributes to advancements in animal husbandry through precision farming, automation, and data analytics.\n",
            "\n",
            "BLEU Score: 0\n",
            "ROUGE Scores:\n",
            "\tROUGE-1 F1 Score: 0.3414634101368233\n",
            "\tROUGE-2 F1 Score: 0.0999999956125002\n",
            "\tROUGE-L F1 Score: 0.3414634101368233\n",
            "\n",
            " ---------------------------------------------------------------\n",
            "INPUT QUERY:\n",
            " Elaborate on the challenges faced in modern animal husbandry practices.\n",
            "\n",
            "TARGET RESPONSE:\n",
            " Modern animal husbandry faces challenges such as disease management, ethical concerns, and environmental impact. Balancing productivity with animal welfare and sustainability is an ongoing challenge for practitioners in the field.\n",
            "\n",
            "LLM RESPONSE:\n",
            " Modern animal husbandry practices face numerous challenges, including genetic and disease issues.\n",
            "\n",
            "BLEU Score: 0\n",
            "ROUGE Scores:\n",
            "\tROUGE-1 F1 Score: 0.24999999580000004\n",
            "\tROUGE-2 F1 Score: 0.09999999601250016\n",
            "\tROUGE-L F1 Score: 0.19999999580000008\n",
            "\n",
            " ---------------------------------------------------------------\n",
            "INPUT QUERY:\n",
            " Discuss the role of nutrition in animal husbandry.\n",
            "\n",
            "TARGET RESPONSE:\n",
            " Nutrition plays a crucial role in animal husbandry as it directly impacts the health, growth, and productivity of livestock. Properly balanced diets ensure optimal development and efficient utilization of nutrients for various purposes, such as milk and meat production.\n",
            "\n",
            "LLM RESPONSE:\n",
            " Nutrition is an essential component of animal husbandry, as it contributes to the health, growth, and productivity of livestock.\n",
            "\n",
            "BLEU Score: 0\n",
            "ROUGE Scores:\n",
            "\tROUGE-1 F1 Score: 0.41509433513705946\n",
            "\tROUGE-2 F1 Score: 0.24999999563775516\n",
            "\tROUGE-L F1 Score: 0.41509433513705946\n",
            "\n",
            "AverageBLEU Score: 0\n",
            "Average ROUGE Scores for 3 samples\n",
            "\tAverage ROUGE-1 F1 Score: 0.3355192470246276\n",
            "\tAverage ROUGE-2 F1 Score: 0.14999999575425185\n",
            "\tAverageROUGE-L F1 Score: 0.31885258035796094\n",
            "\n",
            " ---------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "def compute_rouge_scores(hypotheses, references):\n",
        "    rouge = Rouge()\n",
        "    scores = rouge.get_scores(hypotheses, references, avg=True)\n",
        "    return scores\n",
        "\n",
        "\n",
        "def compute_bleu_score(target_response, llm_responses):\n",
        "    bleu_score = corpus_bleu([target_response.split()], [llm_responses.split()])  # Calculate BLEU score\n",
        "    return bleu_score\n",
        "\n",
        "\n",
        "def Find_Instruction(input_pattern, input_string):\n",
        "    matches = re.findall(input_pattern, input_string, re.DOTALL)\n",
        "\n",
        "    # If there are matches, extract the first one\n",
        "    extracted_string = None\n",
        "    if matches:\n",
        "        extracted_string = matches[0]\n",
        "\n",
        "    return extracted_string\n",
        "\n",
        "\n",
        "def Evaluate(Sample=None, count=0):\n",
        "    print(\"\\n =================================== Evaluation =================================== \")\n",
        "    input_pattern = r'<s>### Instruction:\\n(.*?) \\n'\n",
        "    response_pattern = r'Response:\\n(.*?)</s>'\n",
        "    bleu_scoreS = []\n",
        "    rouge_scoreS = []\n",
        "\n",
        "    if count != 0:\n",
        "        iteration = count - 1\n",
        "    else:\n",
        "        iteration = count\n",
        "\n",
        "    while iteration >= 0:\n",
        "\n",
        "        input_query = Find_Instruction(input_pattern, Sample[iteration][\"inputs\"])\n",
        "        target_response = Find_Instruction(response_pattern, Sample[iteration][\"inputs\"])\n",
        "\n",
        "        if input_query and target_response is not None:\n",
        "            print(\"\\n ---------------------------------------------------------------\")\n",
        "            print(\"INPUT QUERY:\\n\", input_query)\n",
        "            print(\"\\nTARGET RESPONSE:\\n\", target_response)\n",
        "\n",
        "            llm_responses = llm_chain.run(Instruction=f\"{input_query}\")\n",
        "            print(\"\\nLLM RESPONSE:\\n\", llm_responses)\n",
        "\n",
        "            rouge_scores = compute_rouge_scores(llm_responses, target_response)\n",
        "\n",
        "            bleu_score = compute_bleu_score(target_response, llm_responses)\n",
        "            print(\"\\nBLEU Score:\", bleu_score)\n",
        "            print(\"ROUGE Scores:\")\n",
        "            print(\"\\tROUGE-1 F1 Score:\", rouge_scores[\"rouge-1\"][\"f\"])\n",
        "            print(\"\\tROUGE-2 F1 Score:\", rouge_scores[\"rouge-2\"][\"f\"])\n",
        "            print(\"\\tROUGE-L F1 Score:\", rouge_scores[\"rouge-l\"][\"f\"])\n",
        "            rouge_scoreS.append((rouge_scores[\"rouge-1\"][\"f\"], rouge_scores[\"rouge-2\"][\"f\"], rouge_scores[\"rouge-l\"][\"f\"]))\n",
        "            bleu_scoreS.append(bleu_score)\n",
        "\n",
        "\n",
        "        iteration -= 1\n",
        "\n",
        "    if count > 0:\n",
        "        rouge_scores1 = 0\n",
        "        rouge_scores2 = 0\n",
        "        rouge_scores3 = 0\n",
        "        bleu_scoreA = 0\n",
        "\n",
        "        for i in bleu_scoreS:\n",
        "            bleu_scoreA += i\n",
        "        for i in rouge_scoreS:\n",
        "            rouge_scores1 += i[0]\n",
        "            rouge_scores2 += i[1]\n",
        "            rouge_scores3 += i[2]\n",
        "\n",
        "        print(\"\\nAverageBLEU Score:\", bleu_scoreA)\n",
        "        print(f\"Average ROUGE Scores for {count} samples\")\n",
        "        print(\"\\tAverage ROUGE-1 F1 Score:\", rouge_scores1 / count)\n",
        "        print(\"\\tAverage ROUGE-2 F1 Score:\", rouge_scores2 / count)\n",
        "        print(\"\\tAverageROUGE-L F1 Score:\", rouge_scores3 / count)\n",
        "\n",
        "    print(\"\\n ---------------------------------------------------------------\")\n",
        "\n",
        "\n",
        "Evaluate(Sample=train_dataset, count=3)  # one sample evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0oBkpjyuFZlu"
      },
      "source": [
        " ## **7. Intergrating  Retreival-Augmented Generation**\n",
        "\n",
        "\n",
        "\"Retrieval-Augmented Generation\" is a technique used in natural language processing (NLP) and artificial intelligence (AI) that combines elements of both retrieval-based and generation-based models to improve text generation tasks.\n",
        "\n",
        "In traditional text generation tasks, such as language modeling or dialogue generation, the model generates responses solely based on the input context without accessing external knowledge. However, retrieval-augmented generation introduces a retrieval step where the model first retrieves relevant information from a large external knowledge source, such as a database or a corpus of documents, before generating the output.\n",
        "\n",
        "It allows models to leverage external knowledge sources to enhance the quality and relevance of generated text, leading to more accurate and informative outputs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r4f4FrLhGJ7V",
        "outputId": "2a49d097-ecd2-4a3a-946d-9ccbe0416836"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting gradient_haystack==0.2.0\n",
            "  Using cached gradient_haystack-0.2.0-py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: haystack-ai in c:\\users\\swafey\\anaconda3\\lib\\site-packages (from gradient_haystack==0.2.0) (2.2.1)\n",
            "Requirement already satisfied: gradientai>=1.4.0 in c:\\users\\swafey\\anaconda3\\lib\\site-packages (from gradient_haystack==0.2.0) (1.13.0)\n",
            "Requirement already satisfied: urllib3>=1.25.3 in c:\\users\\swafey\\anaconda3\\lib\\site-packages (from gradientai>=1.4.0->gradient_haystack==0.2.0) (1.26.14)\n",
            "Requirement already satisfied: pydantic<3,>=1.10.15 in c:\\users\\swafey\\anaconda3\\lib\\site-packages (from gradientai>=1.4.0->gradient_haystack==0.2.0) (2.7.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\swafey\\anaconda3\\lib\\site-packages (from gradientai>=1.4.0->gradient_haystack==0.2.0) (2.8.2)\n",
            "Requirement already satisfied: aenum>=3.1.11 in c:\\users\\swafey\\anaconda3\\lib\\site-packages (from gradientai>=1.4.0->gradient_haystack==0.2.0) (3.1.15)\n",
            "Requirement already satisfied: more-itertools in c:\\users\\swafey\\anaconda3\\lib\\site-packages (from haystack-ai->gradient_haystack==0.2.0) (10.3.0)\n",
            "Requirement already satisfied: numpy in c:\\users\\swafey\\anaconda3\\lib\\site-packages (from haystack-ai->gradient_haystack==0.2.0) (1.23.5)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\swafey\\anaconda3\\lib\\site-packages (from haystack-ai->gradient_haystack==0.2.0) (3.1.2)\n",
            "Requirement already satisfied: pandas in c:\\users\\swafey\\anaconda3\\lib\\site-packages (from haystack-ai->gradient_haystack==0.2.0) (1.5.3)\n",
            "Requirement already satisfied: tenacity in c:\\users\\swafey\\anaconda3\\lib\\site-packages (from haystack-ai->gradient_haystack==0.2.0) (8.3.0)\n",
            "Requirement already satisfied: networkx in c:\\users\\swafey\\anaconda3\\lib\\site-packages (from haystack-ai->gradient_haystack==0.2.0) (2.8.4)\n",
            "Requirement already satisfied: pyyaml in c:\\users\\swafey\\anaconda3\\lib\\site-packages (from haystack-ai->gradient_haystack==0.2.0) (6.0)\n",
            "Requirement already satisfied: openai>=1.1.0 in c:\\users\\swafey\\anaconda3\\lib\\site-packages (from haystack-ai->gradient_haystack==0.2.0) (1.33.0)\n",
            "Requirement already satisfied: posthog in c:\\users\\swafey\\anaconda3\\lib\\site-packages (from haystack-ai->gradient_haystack==0.2.0) (3.5.0)\n",
            "Requirement already satisfied: requests in c:\\users\\swafey\\anaconda3\\lib\\site-packages (from haystack-ai->gradient_haystack==0.2.0) (2.32.3)\n",
            "Requirement already satisfied: tqdm in c:\\users\\swafey\\anaconda3\\lib\\site-packages (from haystack-ai->gradient_haystack==0.2.0) (4.64.1)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in c:\\users\\swafey\\anaconda3\\lib\\site-packages (from haystack-ai->gradient_haystack==0.2.0) (4.12.2)\n",
            "Requirement already satisfied: lazy-imports in c:\\users\\swafey\\anaconda3\\lib\\site-packages (from haystack-ai->gradient_haystack==0.2.0) (0.3.1)\n",
            "Requirement already satisfied: sniffio in c:\\users\\swafey\\anaconda3\\lib\\site-packages (from openai>=1.1.0->haystack-ai->gradient_haystack==0.2.0) (1.2.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\swafey\\anaconda3\\lib\\site-packages (from openai>=1.1.0->haystack-ai->gradient_haystack==0.2.0) (1.9.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\swafey\\anaconda3\\lib\\site-packages (from openai>=1.1.0->haystack-ai->gradient_haystack==0.2.0) (3.5.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\swafey\\anaconda3\\lib\\site-packages (from openai>=1.1.0->haystack-ai->gradient_haystack==0.2.0) (0.27.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\swafey\\anaconda3\\lib\\site-packages (from pydantic<3,>=1.10.15->gradientai>=1.4.0->gradient_haystack==0.2.0) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.4 in c:\\users\\swafey\\anaconda3\\lib\\site-packages (from pydantic<3,>=1.10.15->gradientai>=1.4.0->gradient_haystack==0.2.0) (2.18.4)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\swafey\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->gradientai>=1.4.0->gradient_haystack==0.2.0) (1.16.0)\n",
            "Requirement already satisfied: colorama in c:\\users\\swafey\\anaconda3\\lib\\site-packages (from tqdm->haystack-ai->gradient_haystack==0.2.0) (0.4.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\swafey\\anaconda3\\lib\\site-packages (from jinja2->haystack-ai->gradient_haystack==0.2.0) (2.1.1)\n",
            "Requirement already satisfied: pytz>=2020.1 in c:\\users\\swafey\\anaconda3\\lib\\site-packages (from pandas->haystack-ai->gradient_haystack==0.2.0) (2022.7)\n",
            "Requirement already satisfied: backoff>=1.10.0 in c:\\users\\swafey\\anaconda3\\lib\\site-packages (from posthog->haystack-ai->gradient_haystack==0.2.0) (2.2.1)\n",
            "Requirement already satisfied: monotonic>=1.5 in c:\\users\\swafey\\anaconda3\\lib\\site-packages (from posthog->haystack-ai->gradient_haystack==0.2.0) (1.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\swafey\\anaconda3\\lib\\site-packages (from requests->haystack-ai->gradient_haystack==0.2.0) (2.0.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\swafey\\anaconda3\\lib\\site-packages (from requests->haystack-ai->gradient_haystack==0.2.0) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\swafey\\anaconda3\\lib\\site-packages (from requests->haystack-ai->gradient_haystack==0.2.0) (2022.12.7)\n",
            "Requirement already satisfied: httpcore==1.* in c:\\users\\swafey\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->openai>=1.1.0->haystack-ai->gradient_haystack==0.2.0) (1.0.5)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\swafey\\anaconda3\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai>=1.1.0->haystack-ai->gradient_haystack==0.2.0) (0.14.0)\n",
            "Installing collected packages: gradient_haystack\n",
            "  Attempting uninstall: gradient_haystack\n",
            "    Found existing installation: gradient-haystack 0.4.0\n",
            "    Uninstalling gradient-haystack-0.4.0:\n",
            "      Successfully uninstalled gradient-haystack-0.4.0\n",
            "Successfully installed gradient_haystack-0.2.0\n"
          ]
        }
      ],
      "source": [
        "!pip install gradient_haystack==0.2.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "kE3KAEc6GLVK"
      },
      "outputs": [],
      "source": [
        "from gradient_haystack.embedders.gradient_document_embedder import GradientDocumentEmbedder\n",
        "from gradient_haystack.embedders.gradient_text_embedder import GradientTextEmbedder\n",
        "from gradient_haystack.generator.base import GradientGenerator\n",
        "from haystack import Document, Pipeline\n",
        "from haystack.components.writers import DocumentWriter\n",
        "from haystack.document_stores.in_memory.document_store import InMemoryDocumentStore\n",
        "from haystack.components.retrievers.in_memory.embedding_retriever import InMemoryEmbeddingRetriever\n",
        "from haystack.components.builders import PromptBuilder\n",
        "from haystack.components.builders.answer_builder import AnswerBuilder\n",
        "import os\n",
        "import requests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "x4rNiS8FGPXd"
      },
      "outputs": [],
      "source": [
        "os.environ['GRADIENT_ACCESS_TOKEN'] = \"4RkXwcXCIhjSilcrkYNanvSI8h1WWrgt\"\n",
        "os.environ['GRADIENT_WORKSPACE_ID'] = \"496b8f01-47f9-4f62-91c8-e634679ca2d3_workspace\"\n",
        "\n",
        "fine_tuned_Model_Id = \"28643f93-bdd5-4602-b911-2e9fea183186_model_adapter\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HSljEN8TIEaK",
        "outputId": "828bdfec-3714-48c7-b05d-26b79cabcac0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "75697\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 1/1 [00:02<00:00,  2.23s/it]\n"
          ]
        }
      ],
      "source": [
        "document_store = InMemoryDocumentStore()\n",
        "writer = DocumentWriter(document_store=document_store)\n",
        "\n",
        "\n",
        "document_embedder = GradientDocumentEmbedder(\n",
        "    access_token=os.environ[\"GRADIENT_ACCESS_TOKEN\"],\n",
        "    workspace_id=os.environ[\"GRADIENT_WORKSPACE_ID\"],\n",
        ")\n",
        "\n",
        "# URL of the online repository where the Raw_Text_Data.txt file is located\n",
        "url = \"https://raw.githubusercontent.com/swafey-karanja/Model-training/main/Raw_Text_Data.txt\"\n",
        "\n",
        "# Send a GET request to download the file\n",
        "response = requests.get(url)\n",
        "\n",
        "# Check if the request was successful (status code 200)\n",
        "if response.status_code == 200:\n",
        "    # Read the contents of the downloaded file\n",
        "    text_data = response.text\n",
        "else:\n",
        "    # If the request was not successful, print an error message\n",
        "    print(\"Failed to download the file from the URL:\", url)\n",
        "\n",
        "docs = [\n",
        "    Document(content=text_data)\n",
        "]\n",
        "\n",
        "print(len(text_data))\n",
        "\n",
        "indexing_pipeline = Pipeline()\n",
        "indexing_pipeline.add_component(instance=document_embedder, name=\"document_embedder\")\n",
        "indexing_pipeline.add_component(instance=writer, name=\"writer\")\n",
        "indexing_pipeline.connect(\"document_embedder\", \"writer\")\n",
        "indexing_pipeline.run({\"document_embedder\": {\"documents\": docs}})\n",
        "\n",
        "text_embedder = GradientTextEmbedder(\n",
        "    access_token=os.environ[\"GRADIENT_ACCESS_TOKEN\"],\n",
        "    workspace_id=os.environ[\"GRADIENT_WORKSPACE_ID\"],\n",
        ")\n",
        "\n",
        "generator = GradientGenerator(\n",
        "    access_token=os.environ[\"GRADIENT_ACCESS_TOKEN\"],\n",
        "    workspace_id=os.environ[\"GRADIENT_WORKSPACE_ID\"],\n",
        "    model_adapter_id=fine_tuned_Model_Id,\n",
        "    max_generated_token_count=350,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "FO4BvUHpIrxp"
      },
      "outputs": [],
      "source": [
        "prompt = \"\"\"You are helpful assistant meant to answer questions relating to animal husbandry. Answer the query, based on the\n",
        "content in the documents. if you dont know the answer respond by saying you are unable to assist with that at the moment.\n",
        "{{documents}}\n",
        "Query: {{query}}\n",
        "\\nAnswer:\n",
        "\"\"\"\n",
        "\n",
        "retriever = InMemoryEmbeddingRetriever(document_store=document_store)\n",
        "prompt_builder = PromptBuilder(template=prompt)\n",
        "\n",
        "rag_pipeline = Pipeline()\n",
        "rag_pipeline.add_component(instance=text_embedder, name=\"text_embedder\")\n",
        "rag_pipeline.add_component(instance=retriever, name=\"retriever\")\n",
        "rag_pipeline.add_component(instance=prompt_builder, name=\"prompt_builder\")\n",
        "rag_pipeline.add_component(instance=generator, name=\"generator\")\n",
        "rag_pipeline.add_component(instance=AnswerBuilder(), name=\"answer_builder\")\n",
        "rag_pipeline.connect(\"generator.replies\", \"answer_builder.replies\")\n",
        "rag_pipeline.connect(\"retriever\", \"answer_builder.documents\")\n",
        "rag_pipeline.connect(\"text_embedder\", \"retriever\")\n",
        "rag_pipeline.connect(\"retriever\", \"prompt_builder.documents\")\n",
        "rag_pipeline.connect(\"prompt_builder\", \"generator\")\n",
        "\n",
        "\n",
        "def LLM_Run(question):\n",
        "    result = rag_pipeline.run(\n",
        "        {\n",
        "            \"text_embedder\": {\"text\": question},\n",
        "            \"prompt_builder\": {\"query\": question},\n",
        "            \"answer_builder\": {\"query\": question}\n",
        "        }\n",
        "    )\n",
        "    return result[\"answer_builder\"][\"answers\"][0].data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DXlb_oNaItUz",
        "outputId": "3d24ca8c-2ab0-4d02-c4d2-bfb9d7f6289e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Diarrhoea is dangerous when it is accompanied by symptoms such as fever, dehydration, and weight loss.\n"
          ]
        }
      ],
      "source": [
        "Query = \"when is diarrhoea dangerous?\"\n",
        "print(LLM_Run(Query))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 426
        },
        "id": "TqhOWDwomTYv",
        "outputId": "91708b52-7828-4a1b-89ec-81ba092d5b0b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting anvil-uplink\n",
            "  Downloading anvil_uplink-0.4.2-py2.py3-none-any.whl (90 kB)\n",
            "\u001b[?25l     \u001b[90m\u001b[0m \u001b[32m0.0/90.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m\u001b[0m \u001b[32m90.1/90.1 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting argparse (from anvil-uplink)\n",
            "  Downloading argparse-1.4.0-py2.py3-none-any.whl (23 kB)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.10/dist-packages (from anvil-uplink) (0.18.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from anvil-uplink) (1.16.0)\n",
            "Collecting ws4py (from anvil-uplink)\n",
            "  Downloading ws4py-0.5.1.tar.gz (51 kB)\n",
            "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m51.4/51.4 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: ws4py\n",
            "  Building wheel for ws4py (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ws4py: filename=ws4py-0.5.1-py3-none-any.whl size=45228 sha256=eb23a86c737c3cc6d4f4a99fd9df1f79111d833213d48a13ba3342b4c2010974\n",
            "  Stored in directory: /root/.cache/pip/wheels/2e/7c/ad/d9c746276bf024d44296340869fcb169f1e5d80fb147351a57\n",
            "Successfully built ws4py\n",
            "Installing collected packages: ws4py, argparse, anvil-uplink\n",
            "Successfully installed anvil-uplink-0.4.2 argparse-1.4.0 ws4py-0.5.1\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "id": "af179c19035c4ed1a108f277f156aa7b",
              "pip_warning": {
                "packages": [
                  "argparse",
                  "google"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "!pip install anvil-uplink"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P4_bgYoymwmX"
      },
      "outputs": [],
      "source": [
        "import anvil.server"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lk11zvaqm6Gz",
        "outputId": "0083ac16-59b5-4c1d-e1ef-d81c125f2499"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Connecting to wss://anvil.works/uplink\n",
            "Anvil websocket open\n",
            "Connected to \"Default Environment\" as SERVER\n"
          ]
        }
      ],
      "source": [
        "anvil.server.connect(\"server_5A33FQHS5NVTR2BFISKONOLE-UOGPLND5WG64J4TQ\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "umew5Tuor32l"
      },
      "outputs": [],
      "source": [
        "@anvil.server.callable\n",
        "def question_answer(question):\n",
        "  response = LLM_Run(question)\n",
        "  print(response)\n",
        "  return response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "jzQEr0QXuSyp",
        "outputId": "d85bc2ca-26e9-4fea-dc62-a23b54945ce5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The issue could be warts or calluses.\n",
            "Lumps on the skin of cattle can be caused by various factors such as insect bites, skin infections, or tumors. It is essential to monitor the lumps for changes in size, shape, or texture. If the lumps are large, painful, or appear to be growing, it is recommended to seek professional help. A veterinarian can diagnose the cause of the lumps and provide appropriate treatment.\n"
          ]
        }
      ],
      "source": [
        "anvil.server.wait_forever()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
