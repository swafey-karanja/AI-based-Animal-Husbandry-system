{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zeANMo5VPYYo"
      },
      "source": [
        "# **VET-ASSISTANT LLM**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zu96hl0nOwUk"
      },
      "source": [
        "**Installing dependencies**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Un-Qup9eNRq3",
        "outputId": "c8c2d3ae-fbd6-43d4-d688-092572d03f85"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gradientai in /usr/local/lib/python3.10/dist-packages (1.13.0)\n",
            "Requirement already satisfied: aenum>=3.1.11 in /usr/local/lib/python3.10/dist-packages (from gradientai) (3.1.15)\n",
            "Requirement already satisfied: pydantic<3,>=1.10.15 in /usr/local/lib/python3.10/dist-packages (from gradientai) (2.7.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from gradientai) (2.8.2)\n",
            "Requirement already satisfied: urllib3>=1.25.3 in /usr/local/lib/python3.10/dist-packages (from gradientai) (2.0.7)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.10.15->gradientai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.10.15->gradientai) (2.18.4)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.10.15->gradientai) (4.12.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->gradientai) (1.16.0)\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.10/dist-packages (0.2.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.30)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.9.5)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Requirement already satisfied: langchain-core<0.3.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.2.5)\n",
            "Requirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.2.1)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.1.75)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.25.2)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.7.3)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.3.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.0->langchain) (1.33)\n",
            "Requirement already satisfied: packaging<24.0,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.0->langchain) (23.2)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.10.3)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (2.18.4)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (4.12.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2024.6.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.0->langchain) (2.4)\n",
            "Requirement already satisfied: gradient_haystack in /usr/local/lib/python3.10/dist-packages (0.4.0)\n",
            "Requirement already satisfied: gradientai>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from gradient_haystack) (1.13.0)\n",
            "Requirement already satisfied: haystack-ai in /usr/local/lib/python3.10/dist-packages (from gradient_haystack) (2.2.1)\n",
            "Requirement already satisfied: aenum>=3.1.11 in /usr/local/lib/python3.10/dist-packages (from gradientai>=1.4.0->gradient_haystack) (3.1.15)\n",
            "Requirement already satisfied: pydantic<3,>=1.10.15 in /usr/local/lib/python3.10/dist-packages (from gradientai>=1.4.0->gradient_haystack) (2.7.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from gradientai>=1.4.0->gradient_haystack) (2.8.2)\n",
            "Requirement already satisfied: urllib3>=1.25.3 in /usr/local/lib/python3.10/dist-packages (from gradientai>=1.4.0->gradient_haystack) (2.0.7)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from haystack-ai->gradient_haystack) (3.1.4)\n",
            "Requirement already satisfied: lazy-imports in /usr/local/lib/python3.10/dist-packages (from haystack-ai->gradient_haystack) (0.3.1)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.10/dist-packages (from haystack-ai->gradient_haystack) (10.1.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from haystack-ai->gradient_haystack) (3.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from haystack-ai->gradient_haystack) (1.25.2)\n",
            "Requirement already satisfied: openai>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from haystack-ai->gradient_haystack) (1.33.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from haystack-ai->gradient_haystack) (2.0.3)\n",
            "Requirement already satisfied: posthog in /usr/local/lib/python3.10/dist-packages (from haystack-ai->gradient_haystack) (3.5.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from haystack-ai->gradient_haystack) (6.0.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from haystack-ai->gradient_haystack) (2.32.3)\n",
            "Requirement already satisfied: tenacity in /usr/local/lib/python3.10/dist-packages (from haystack-ai->gradient_haystack) (8.3.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from haystack-ai->gradient_haystack) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from haystack-ai->gradient_haystack) (4.12.1)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai>=1.1.0->haystack-ai->gradient_haystack) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai>=1.1.0->haystack-ai->gradient_haystack) (1.7.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai>=1.1.0->haystack-ai->gradient_haystack) (0.27.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai>=1.1.0->haystack-ai->gradient_haystack) (1.3.1)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.10.15->gradientai>=1.4.0->gradient_haystack) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.10.15->gradientai>=1.4.0->gradient_haystack) (2.18.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->gradientai>=1.4.0->gradient_haystack) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->haystack-ai->gradient_haystack) (2.1.5)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->haystack-ai->gradient_haystack) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->haystack-ai->gradient_haystack) (2024.1)\n",
            "Requirement already satisfied: monotonic>=1.5 in /usr/local/lib/python3.10/dist-packages (from posthog->haystack-ai->gradient_haystack) (1.6)\n",
            "Requirement already satisfied: backoff>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from posthog->haystack-ai->gradient_haystack) (2.2.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->haystack-ai->gradient_haystack) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->haystack-ai->gradient_haystack) (3.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->haystack-ai->gradient_haystack) (2024.6.2)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai>=1.1.0->haystack-ai->gradient_haystack) (1.2.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai>=1.1.0->haystack-ai->gradient_haystack) (1.0.5)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai>=1.1.0->haystack-ai->gradient_haystack) (0.14.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (2024.5.15)\n",
            "Requirement already satisfied: rouge in /usr/local/lib/python3.10/dist-packages (1.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from rouge) (1.16.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.5.15)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.4)\n",
            "Requirement already satisfied: wandb in /usr/local/lib/python3.10/dist-packages (0.17.1)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.1.43)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from wandb) (4.2.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<6,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.1)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.32.3)\n",
            "Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.5.1)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.10/dist-packages (from wandb) (1.3.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (67.7.2)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.11)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2024.6.2)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.19.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.14.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.25.2)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (14.0.2)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.0.3)\n",
            "Requirement already satisfied: requests>=2.32.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.4)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec[http]<=2024.3.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.5)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.23.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.2->datasets) (4.12.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.1->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.1->datasets) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.1->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.1->datasets) (2024.6.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Requirement already satisfied: rouge in /usr/local/lib/python3.10/dist-packages (1.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from rouge) (1.16.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.5.15)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.4)\n",
            "Collecting langchain_community\n",
            "  Downloading langchain_community-0.2.4-py3-none-any.whl (2.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (2.0.30)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (3.9.5)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain_community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Requirement already satisfied: langchain<0.3.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (0.2.3)\n",
            "Requirement already satisfied: langchain-core<0.3.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (0.2.5)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (0.1.75)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (1.25.2)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (2.32.3)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (8.3.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (4.0.3)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading marshmallow-3.21.3-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.2/49.2 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Requirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from langchain<0.3.0,>=0.2.0->langchain_community) (0.2.1)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain<0.3.0,>=0.2.0->langchain_community) (2.7.3)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.0->langchain_community) (1.33)\n",
            "Requirement already satisfied: packaging<24.0,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.0->langchain_community) (23.2)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.0->langchain_community) (3.10.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain_community) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain_community) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain_community) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain_community) (2024.6.2)\n",
            "Requirement already satisfied: typing-extensions>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain_community) (4.12.1)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain_community) (3.0.3)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.0->langchain_community) (2.4)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain<0.3.0,>=0.2.0->langchain_community) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain<0.3.0,>=0.2.0->langchain_community) (2.18.4)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: mypy-extensions, marshmallow, typing-inspect, dataclasses-json, langchain_community\n",
            "Successfully installed dataclasses-json-0.6.7 langchain_community-0.2.4 marshmallow-3.21.3 mypy-extensions-1.0.0 typing-inspect-0.9.0\n"
          ]
        }
      ],
      "source": [
        "!pip install gradientai --upgrade\n",
        "!pip install langchain\n",
        "!pip install -U gradient_haystack\n",
        "!pip install regex\n",
        "!pip install rouge\n",
        "!pip install nltk\n",
        "!pip install wandb\n",
        "!pip install datasets\n",
        "!pip install rouge\n",
        "!pip install nltk\n",
        "!pip install langchain_community\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ids19xUiPP_A"
      },
      "source": [
        "## **1.Loading the Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OWjkVgavPkjM",
        "outputId": "2e21bb03-cfb0-4d08-a8d8-acffb200e4a6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset Size: 1412\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import requests\n",
        "\n",
        "# URL of the online repository where the dataset is hosted\n",
        "dataset_url = \"https://raw.githubusercontent.com/swafey-karanja/Model-training/main/NewData.json\"\n",
        "\n",
        "# Make an HTTP GET request to fetch the dataset\n",
        "response = requests.get(dataset_url)\n",
        "\n",
        "# Check if the request was successful (status code 200)\n",
        "if response.status_code == 200:\n",
        "    # Load the dataset from the response content\n",
        "    train_dataset = json.loads(response.text)\n",
        "\n",
        "    # Print the size of the dataset\n",
        "    print(\"Dataset Size:\", len(train_dataset))\n",
        "else:\n",
        "    # Print an error message if the request failed\n",
        "    print(\"Failed to fetch dataset. Status code:\", response.status_code)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KnzGaOMVpGf4"
      },
      "source": [
        "**Break the data into batches**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DYzINIs6pErW",
        "outputId": "8d345e11-1749-4471-d2ec-12dc23dfb03e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batches size\n",
            "[100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 12]\n"
          ]
        }
      ],
      "source": [
        "def divide_into_Batches(number, chunk_size):  # Define a function to divide a number into chunks of a given size\n",
        "    Batches = []\n",
        "    while number > 0:\n",
        "        if number >= chunk_size:\n",
        "            Batches.append(chunk_size)\n",
        "            number -= chunk_size\n",
        "        else:\n",
        "            Batches.append(number)\n",
        "            break\n",
        "\n",
        "    return Batches\n",
        "\n",
        "Batches = divide_into_Batches(len(train_dataset), 100)  # Divide the dataset into chunks of 100 samples each\n",
        "print(\"Batches size\")\n",
        "print(Batches)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "apmpbpeppW2T"
      },
      "source": [
        "## **2.Load the Base Model.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I-W4W_s3pySl"
      },
      "source": [
        "Loading the Nous-Hermes-Llama2-13b base model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uPy0iC61pj1o"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ['GRADIENT_ACCESS_TOKEN'] = \"Y2XG6OMr6oZbZETWKkPKogmWshFlD5ft\"\n",
        "os.environ['GRADIENT_WORKSPACE_ID'] = \"c571b959-4ce8-474a-b3f3-398c7b347c57_workspace\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UOarEjaEpvWX",
        "outputId": "39907253-f18d-4b1d-8a53-c7c87e9d36cd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Available Base Models\n",
            "\t <gradientai._base_model.BaseModel object at 0x7b722633d2a0>\n",
            "\t <gradientai._base_model.BaseModel object at 0x7b7224a41d80>\n",
            "\t <gradientai._base_model.BaseModel object at 0x7b7224a40ca0>\n",
            "\t <gradientai._base_model.BaseModel object at 0x7b7224a42d10>\n",
            "\t <gradientai._base_model.BaseModel object at 0x7b7224a418d0>\n",
            "\t <gradientai._base_model.BaseModel object at 0x7b7224a414b0>\n",
            "\t <gradientai._base_model.BaseModel object at 0x7b7224a41270>\n",
            "\n",
            "Base Model Chosen : <gradientai._base_model.BaseModel object at 0x7b7224a40d60>\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import os\n",
        "from gradientai import Gradient\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.llms import GradientLLM\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "\n",
        "os.environ['GRADIENT_ACCESS_TOKEN'] = \"Y2XG6OMr6oZbZETWKkPKogmWshFlD5ft\"\n",
        "os.environ['GRADIENT_WORKSPACE_ID'] = \"c571b959-4ce8-474a-b3f3-398c7b347c57_workspace\"\n",
        "\n",
        "gradient =  Gradient()\n",
        "\n",
        "print(\"Available Base Models\")\n",
        "for i in gradient.list_models(only_base=True):\n",
        "    print(\"\\t\",i)\n",
        "\n",
        "base_model_id = \"NousResearch/Nous-Hermes-Llama2-13b\"\n",
        "base_model_name = \"nous-hermes2\"\n",
        "base_model = gradient.get_base_model(base_model_slug=\"nous-hermes2\") # base model Nous-Hermes-Llama2-13b\n",
        "\n",
        "print(\"\\nBase Model Chosen :\", base_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G2WHiZMvtxGg"
      },
      "source": [
        "##**3. Creating a Model Adapter**\n",
        "\n",
        "* Adapters are small, lightweight modules inserted between\n",
        "existing layers of a pre-trained LLM. They act like \"add-ons\" that focus on learning task-specific information without modifying the core knowledge captured in the original model.\n",
        "\n",
        "* The addapter server as the object that we are going to fine tune"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zKK8fwv5t_mX",
        "outputId": "45e7be51-95c1-4ca7-a213-f035b8b75510"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Base model id                : cc2dafce-9e6e-4a23-a918-cad6ba89e42e_base_ml_model\n",
            "Fine tune model Name         : Llama2-13b/Vet-assistant-Newdata\n",
            "Fine tune model adapter id   : 2e7a849b-239e-44a1-a88a-3be1d77c1f93_model_adapter\n",
            "\n",
            "\n",
            "\n",
            "Size of object in memory, in bytes. 56\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'_api_instance': <gradientai.openapi.client.api.models_api.ModelsApi at 0x7b7224a41630>,\n",
              " '_id': '2e7a849b-239e-44a1-a88a-3be1d77c1f93_model_adapter',\n",
              " '_workspace_id': 'c571b959-4ce8-474a-b3f3-398c7b347c57_workspace',\n",
              " '_async_semaphore': <asyncio.locks.Semaphore object at 0x7b7224a42680 [unlocked, value:8]>,\n",
              " '_base_model_id': 'cc2dafce-9e6e-4a23-a918-cad6ba89e42e_base_ml_model',\n",
              " '_name': 'Llama2-13b/Vet-assistant-Newdata'}"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "our_finetune_model_name=\"Llama2-13b/Vet-assistant-Newdata\"\n",
        "Fine_Tune__adapter = base_model.create_model_adapter(\n",
        "        name=our_finetune_model_name, # base mode nous hermis\n",
        "        learning_rate=0.00005, #  Determines how fast a model updates its knowledge during fine-tuning.\n",
        "        rank=8,  # Dimensionality\n",
        "\n",
        "    )\n",
        "\n",
        "# default hyperparameters Frozen\n",
        "hyperparameters = {\n",
        "                  \"block_size\": 1024,\n",
        "                  \"model_max_length\": 2048,\n",
        "                  \"padding\": \"right\",\n",
        "                  \"use_flash_attention_2\": False,\n",
        "                  \"disable_gradient_checkpointing\": False,\n",
        "                  \"logging_steps\": -1,\n",
        "                  \"evaluation_strategy\": \"epoch\",\n",
        "                  \"save_total_limit\": 1,\n",
        "                  \"save_strategy\": \"epoch\",\n",
        "                  \"auto_find_batch_size\": False,\n",
        "                  \"mixed_precision\": \"fp16\",\n",
        "                  \"epochs\": 3,\n",
        "                  \"batch_size\": 100,\n",
        "                  \"warmup_ratio\": 0.1,\n",
        "                  \"gradient_accumulation\": 1,\n",
        "                  \"optimizer\": \"adamw_torch\",\n",
        "                  \"scheduler\": \"linear\",\n",
        "                  \"weight_decay\": 0,\n",
        "                  \"max_grad_norm\": 1,\n",
        "                  \"seed\": 42,\n",
        "                  \"apply_chat_template\": False,\n",
        "                  \"quantization\": \"int4\",\n",
        "                  \"target_modules\": \"\",\n",
        "                  \"merge_adapter\": False,\n",
        "                  \"peft\": True,\n",
        "                  \"lora_r\": 16,\n",
        "                  \"lora_alpha\": 32,\n",
        "                  \"lora_dropout\": 0.05\n",
        "  }\n",
        "\n",
        "\n",
        "print(f\"Base model id                : {Fine_Tune__adapter._base_model_id}\")\n",
        "print(f\"Fine tune model Name         : { Fine_Tune__adapter.name}\")\n",
        "print(f\"Fine tune model adapter id   : {Fine_Tune__adapter.id}\")\n",
        "\n",
        "print(\"\\n\\n\")\n",
        "print(\"Size of object in memory, in bytes.\", Fine_Tune__adapter.__format__.__sizeof__()) # Size of object in memory, in bytes.\n",
        "Fine_Tune__adapter.__dict__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fPU-mOK0uzfF"
      },
      "source": [
        "## **4. Fine Tuning The Adaptor**\n",
        "\n",
        "For this case we will be performing Laura-based finetuning which means that we are freezing about 99% of the layers and then finetuning an adapter on top of it.\n",
        "\n",
        "LoRA: Low-Rank Adaptation of Large Language Models is a novel technique introduced by Microsoft researchers to deal with the problem of fine-tuning large-language models with billions of parameters\n",
        "\n",
        " LoRA proposes to freeze pre-trained model weights and inject trainable layers (rank-decomposition matrices) in each transformer block. This greatly reduces the number of trainable parameters and GPU memory requirements since gradients don't need to be computed for most model weights\n",
        "\n",
        "\n",
        "why LoRA finetuning:\n",
        "\n",
        "1. Faster Training: Since only the added task-specific layers are trained while the pre-trained model's parameters remain frozen, the fine-tuning process is generally faster compared to training a model from scratch\n",
        "2. Computation requirements are lower. We could create a full fine-tuned model in a 2080 Ti with 11 GB of VRAM!\n",
        "3. Trained weights are  much smaller. Because the original model is frozen and we inject new layers to be trained\n",
        "\n",
        " [for more info](https://huggingface.co/blog/lora)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JhFoAaE5vPAk",
        "outputId": "93a47e79-6536-4ea5-e1f1-312e453ca2eb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Our Model id:  2e7a849b-239e-44a1-a88a-3be1d77c1f93_model_adapter\n",
            "================================================================\n",
            "\n",
            "Fine tuning . . .\n",
            "\n",
            "Fine-tuning the model, iteration 1\n",
            "Batch 1 range: 0 : 100\n",
            "\t Batch 1 Evaluation : number_of_trainable_tokens=8007 sum_loss=18044.285\n",
            "Batch 2 range: 100 : 200\n",
            "\t Batch 2 Evaluation : number_of_trainable_tokens=8043 sum_loss=11213.263\n",
            "Batch 3 range: 200 : 300\n",
            "\t Batch 3 Evaluation : number_of_trainable_tokens=9278 sum_loss=10536.057\n",
            "Batch 4 range: 300 : 400\n",
            "\t Batch 4 Evaluation : number_of_trainable_tokens=10454 sum_loss=11967.004\n",
            "Batch 5 range: 400 : 500\n",
            "\t Batch 5 Evaluation : number_of_trainable_tokens=13087 sum_loss=13200.826\n",
            "Batch 6 range: 500 : 600\n",
            "\t Batch 6 Evaluation : number_of_trainable_tokens=16332 sum_loss=17247.604\n",
            "Batch 7 range: 600 : 700\n",
            "\t Batch 7 Evaluation : number_of_trainable_tokens=18421 sum_loss=17550.602\n",
            "Batch 8 range: 700 : 800\n",
            "\t Batch 8 Evaluation : number_of_trainable_tokens=18931 sum_loss=17459.293\n",
            "Batch 9 range: 800 : 900\n",
            "\t Batch 9 Evaluation : number_of_trainable_tokens=13457 sum_loss=11873.373\n",
            "Batch 10 range: 900 : 1000\n",
            "\t Batch 10 Evaluation : number_of_trainable_tokens=10560 sum_loss=9307.085\n",
            "Batch 11 range: 1000 : 1100\n",
            "\t Batch 11 Evaluation : number_of_trainable_tokens=9856 sum_loss=6688.8364\n",
            "Batch 12 range: 1100 : 1200\n",
            "\t Batch 12 Evaluation : number_of_trainable_tokens=8558 sum_loss=6130.7583\n",
            "Batch 13 range: 1200 : 1300\n",
            "\t Batch 13 Evaluation : number_of_trainable_tokens=5507 sum_loss=5665.937\n",
            "Batch 14 range: 1300 : 1400\n",
            "\t Batch 14 Evaluation : number_of_trainable_tokens=5389 sum_loss=5073.09\n",
            "Batch 15 range: 1400 : 1412\n",
            "\t Batch 15 Evaluation : number_of_trainable_tokens=999 sum_loss=954.4346\n"
          ]
        }
      ],
      "source": [
        "print(f\"Our Model id:  {Fine_Tune__adapter.id}\")\n",
        "num_epochs = 1  # num_epochs is the number of times you fine-tune the model # more epochs tends to get better results, but you also run the risk of \"overfitting\"\n",
        "count = 0\n",
        "print(\"================================================================\\n\")\n",
        "print(\"Fine tuning . . .\\n\")\n",
        "while count < num_epochs:\n",
        "    print(f\"Fine-tuning the model, iteration {count + 1}\")\n",
        "    s = 0\n",
        "    n = 1\n",
        "    for Batch in Batches:\n",
        "        print(f\"Batch {n} range: {s} : {(s + Batch)}\")\n",
        "\n",
        "        # Try to fine-tune the model with the chunk of samples,\n",
        "        while True:\n",
        "            try:\n",
        "                metric = Fine_Tune__adapter.fine_tune(samples=train_dataset[s: s + Batch])\n",
        "                print(f\"\\t Batch {n} Evaluation :\", metric)\n",
        "                break\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "\n",
        "\n",
        "        s += Batch\n",
        "        n += 1\n",
        "    count = count + 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JH8ghczL_Xy6"
      },
      "source": [
        "## **5.Model Inference**\n",
        "\n",
        "\"model inference\" typically refers to the process of using a trained model to make predictions on new, unseen data. Fine-tuning a model involves taking a pre-trained model and further training it on a specific task or dataset to improve its performance.\n",
        "\n",
        "When fine-tuning a model, the process of inference remains the same as with any other trained model. Once the fine-tuning is complete, you can use the model to make predictions on new data by passing the data through the model and obtaining the output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fsGJw4hw_8O5"
      },
      "outputs": [],
      "source": [
        "from langchain.chains import LLMChain    # Import the LLMChain class for building LLM-based workflows\n",
        "from langchain.llms import GradientLLM   # Import the GradientLLM class for interacting with Gradient AI's API\n",
        "from langchain.prompts import PromptTemplate # Import the PromptTemplate class for defining how to prompt the LLM\n",
        "import gradientai\n",
        "import os # Import the os module for potential file system interactions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Vbg5iMnAA1-"
      },
      "outputs": [],
      "source": [
        "Fine_Tune__adapter_ID = \"2e7a849b-239e-44a1-a88a-3be1d77c1f93_model_adapter\"\n",
        "#Fine_Tune__adapter_ID = Fine_Tune__adapter.id\n",
        "#  creating a GradientLLM object\n",
        "llm = GradientLLM(\n",
        "    model=Fine_Tune__adapter_ID,\n",
        "    model_kwargs=dict(\n",
        "        max_generated_token_count=128, # Adjust how your model generates completions\n",
        "        temperature = 0.7, # randomness\n",
        "        top_k=50 # Restricts the model to pick from k most likely words,\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vVCG5MR7Au16"
      },
      "source": [
        "### **Formatting Prompts**\n",
        "\n",
        "The model follows the Alpaca prompt format which provides a structured way to input information to the model, guiding it on what kind of output is desired."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fzevoH6vBLXc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0e0e7aae-62cf-4fac-8749-e199262db22f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 0.3.0. Use RunnableSequence, e.g., `prompt | llm` instead.\n",
            "  warn_deprecated(\n"
          ]
        }
      ],
      "source": [
        "template = \"\"\"### Instruction: {Instruction} \\n\\n### Response:\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(template=template, input_variables=[\"Instruction\"])\n",
        "\n",
        "llm_chain = LLMChain(prompt=prompt, llm=llm)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vg0GwPNHBmjo"
      },
      "source": [
        "#### *Example* *inputs* *and* *corresponding* *outputs*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hcTdHlRZBcN7",
        "outputId": "795f11ac-b395-47d5-99b8-47043f17d66f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question :\n",
            " How does Clostridial Diseases impact milk production, reproductive performance, and overall well-being in cows?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 0.3.0. Use invoke instead.\n",
            "  warn_deprecated(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer :\n",
            " Clostridial Diseases can cause reduced milk production, impaired reproductive performance, and decreased overall well-being in cows. \n",
            "\n",
            "\n",
            "Question :\n",
            " what are some of the risk factors associated with lameness in dairy cows?\n",
            "Answer :\n",
            " Risk factors associated with lameness in dairy cows include digital dermatitis, foot rot, and claw diseases. \n",
            "\n",
            "\n",
            "Question :\n",
            " How dose lameness impact mill production, reproduction and overall well-being in cows\n",
            "Answer :\n",
            " Lameness impacts mill production, reproduction, and overall well-being in cows, causing reduced milk production, decreased reproductive success, and decreased body condition. \n",
            "\n",
            "\n",
            "Question :\n",
            " What diseases are prevelant in dairy small ruminant, and what managment practice can mitigate their impact \n",
            "Answer :\n",
            " Diseases prevalent in dairy small ruminant include mastitis, foot rot, and scabies. Management practices such as regular hoof trimming, maintaining clean and dry living conditions, and treating infected animals can mitigate their impact. \n",
            "\n",
            "\n",
            "Question :\n",
            " what specific health managment strategies should be implemented to prevent or treat common cow diseases?\n",
            "Answer :\n",
            " Implement specific health management strategies such as proper nutrition, vaccination programs, and disease surveillance to prevent or treat common cow diseases. \n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "Question1 = \"How does Clostridial Diseases impact milk production, reproductive performance, and overall well-being in cows?\"\n",
        "Question2 = \"what are some of the risk factors associated with lameness in dairy cows?\"\n",
        "Question3 = \"How dose lameness impact mill production, reproduction and overall well-being in cows\"\n",
        "Question4 = \"What diseases are prevelant in dairy small ruminant, and what managment practice can mitigate their impact \"\n",
        "Question5 = \"what specific health managment strategies should be implemented to prevent or treat common cow diseases?\"\n",
        "\n",
        "\n",
        "print(\"Question :\\n\", Question1)\n",
        "Answer = llm_chain.run(Instruction=f\"{Question1}\")\n",
        "print(\"Answer :\\n\", Answer, \"\\n\\n\")\n",
        "\n",
        "print(\"Question :\\n\", Question2)\n",
        "Answer = llm_chain.run(Instruction=f\"{Question2}\")\n",
        "print(\"Answer :\\n\", Answer, \"\\n\\n\")\n",
        "\n",
        "print(\"Question :\\n\", Question3)\n",
        "Answer = llm_chain.run(Instruction=f\"{Question3}\")\n",
        "print(\"Answer :\\n\", Answer, \"\\n\\n\")\n",
        "\n",
        "print(\"Question :\\n\", Question4)\n",
        "Answer = llm_chain.run(Instruction=f\"{Question4}\")\n",
        "print(\"Answer :\\n\", Answer, \"\\n\\n\")\n",
        "\n",
        "print(\"Question :\\n\", Question5)\n",
        "Answer = llm_chain.run(Instruction=f\"{Question5}\")\n",
        "print(\"Answer :\\n\", Answer, \"\\n\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sCh6YPY-DPh8"
      },
      "source": [
        "## **6.Model Evaluation**\n",
        "\n",
        "Here we are using two popular automatic evaluation metrics to assess the performance of your LLM:\n",
        "\n",
        "* BLEU score: This metric calculates the n-gram precision between the generated response and the reference response\n",
        "\n",
        "* ROUGE score: This metric measures the overlap in word n-grams and longest common subsequences between the generated response and the reference response.\n",
        "\n",
        "BLEU and ROUGE scores are calculated to compare the generated response with the target response."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MxVdOqQJEZNS"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import json\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "from rouge import Rouge\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.llms import GradientLLM\n",
        "from langchain.prompts import PromptTemplate\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sHfeZ8FfEbXp",
        "outputId": "8dc2733d-9504-4d06-9e79-9a37b2c07044"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " =================================== Evaluation =================================== \n",
            "\n",
            " ---------------------------------------------------------------\n",
            "INPUT QUERY:\n",
            " How does technology contribute to advancements in animal husbandry?\n",
            "\n",
            "TARGET RESPONSE:\n",
            " Technology in animal husbandry includes innovations like automated feeding systems, precision breeding techniques, and health monitoring devices. These advancements enhance efficiency, reduce costs, and improve overall management practices.\n",
            "\n",
            "LLM RESPONSE:\n",
            " Technology contributes to advancements in animal husbandry by providing tools for data collection and analysis, enhancing productivity and efficiency, and facilitating communication and information sharing.\n",
            "\n",
            "BLEU Score: 0\n",
            "ROUGE Scores:\n",
            "\tROUGE-1 F1 Score: 0.2857142807663474\n",
            "\tROUGE-2 F1 Score: 0.07843136756632098\n",
            "\tROUGE-L F1 Score: 0.2857142807663474\n",
            "\n",
            " ---------------------------------------------------------------\n",
            "INPUT QUERY:\n",
            " Elaborate on the challenges faced in modern animal husbandry practices.\n",
            "\n",
            "TARGET RESPONSE:\n",
            " Modern animal husbandry faces challenges such as disease management, ethical concerns, and environmental impact. Balancing productivity with animal welfare and sustainability is an ongoing challenge for practitioners in the field.\n",
            "\n",
            "LLM RESPONSE:\n",
            " Challenges in modern animal husbandry practices include maintaining genetic diversity and managing disease outbreaks.\n",
            "\n",
            "BLEU Score: 0\n",
            "ROUGE Scores:\n",
            "\tROUGE-1 F1 Score: 0.23809523365079371\n",
            "\tROUGE-2 F1 Score: 0.04761904334467159\n",
            "\tROUGE-L F1 Score: 0.14285713841269856\n",
            "\n",
            " ---------------------------------------------------------------\n",
            "INPUT QUERY:\n",
            " Discuss the role of nutrition in animal husbandry.\n",
            "\n",
            "TARGET RESPONSE:\n",
            " Nutrition plays a crucial role in animal husbandry as it directly impacts the health, growth, and productivity of livestock. Properly balanced diets ensure optimal development and efficient utilization of nutrients for various purposes, such as milk and meat production.\n",
            "\n",
            "LLM RESPONSE:\n",
            "  Nutrition plays a critical role in animal husbandry. Good nutrition ensures proper growth and development of livestock, enhances their productivity, and contributes to their overall health.\n",
            "\n",
            "BLEU Score: 8.06798322521923e-232\n",
            "ROUGE Scores:\n",
            "\tROUGE-1 F1 Score: 0.3389830460212583\n",
            "\tROUGE-2 F1 Score: 0.15873015394305884\n",
            "\tROUGE-L F1 Score: 0.30508474093651256\n",
            "\n",
            "AverageBLEU Score: 8.06798322521923e-232\n",
            "Average ROUGE Scores for 3 samples\n",
            "\tAverage ROUGE-1 F1 Score: 0.2875975201461331\n",
            "\tAverage ROUGE-2 F1 Score: 0.09492685495135046\n",
            "\tAverageROUGE-L F1 Score: 0.24455205337185282\n",
            "\n",
            " ---------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
            "The hypothesis contains 0 counts of 2-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
            "The hypothesis contains 0 counts of 3-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
            "The hypothesis contains 0 counts of 4-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "def compute_rouge_scores(hypotheses, references):\n",
        "    rouge = Rouge()\n",
        "    scores = rouge.get_scores(hypotheses, references, avg=True)\n",
        "    return scores\n",
        "\n",
        "\n",
        "def compute_bleu_score(target_response, llm_responses):\n",
        "    bleu_score = corpus_bleu([target_response.split()], [llm_responses.split()])  # Calculate BLEU score\n",
        "    return bleu_score\n",
        "\n",
        "\n",
        "def Find_Instruction(input_pattern, input_string):\n",
        "    matches = re.findall(input_pattern, input_string, re.DOTALL)\n",
        "\n",
        "    # If there are matches, extract the first one\n",
        "    extracted_string = None\n",
        "    if matches:\n",
        "        extracted_string = matches[0]\n",
        "\n",
        "    return extracted_string\n",
        "\n",
        "\n",
        "def Evaluate(Sample=None, count=0):\n",
        "    print(\"\\n =================================== Evaluation =================================== \")\n",
        "    input_pattern = r'<s>### Instruction:\\n(.*?) \\n'\n",
        "    response_pattern = r'Response:\\n(.*?)</s>'\n",
        "    bleu_scoreS = []\n",
        "    rouge_scoreS = []\n",
        "\n",
        "    if count != 0:\n",
        "        iteration = count - 1\n",
        "    else:\n",
        "        iteration = count\n",
        "\n",
        "    while iteration >= 0:\n",
        "\n",
        "        input_query = Find_Instruction(input_pattern, Sample[iteration][\"inputs\"])\n",
        "        target_response = Find_Instruction(response_pattern, Sample[iteration][\"inputs\"])\n",
        "\n",
        "        if input_query and target_response is not None:\n",
        "            print(\"\\n ---------------------------------------------------------------\")\n",
        "            print(\"INPUT QUERY:\\n\", input_query)\n",
        "            print(\"\\nTARGET RESPONSE:\\n\", target_response)\n",
        "\n",
        "            llm_responses = llm_chain.run(Instruction=f\"{input_query}\")\n",
        "            print(\"\\nLLM RESPONSE:\\n\", llm_responses)\n",
        "\n",
        "            rouge_scores = compute_rouge_scores(llm_responses, target_response)\n",
        "\n",
        "            bleu_score = compute_bleu_score(target_response, llm_responses)\n",
        "            print(\"\\nBLEU Score:\", bleu_score)\n",
        "            print(\"ROUGE Scores:\")\n",
        "            print(\"\\tROUGE-1 F1 Score:\", rouge_scores[\"rouge-1\"][\"f\"])\n",
        "            print(\"\\tROUGE-2 F1 Score:\", rouge_scores[\"rouge-2\"][\"f\"])\n",
        "            print(\"\\tROUGE-L F1 Score:\", rouge_scores[\"rouge-l\"][\"f\"])\n",
        "            rouge_scoreS.append((rouge_scores[\"rouge-1\"][\"f\"], rouge_scores[\"rouge-2\"][\"f\"], rouge_scores[\"rouge-l\"][\"f\"]))\n",
        "            bleu_scoreS.append(bleu_score)\n",
        "\n",
        "\n",
        "        iteration -= 1\n",
        "\n",
        "    if count > 0:\n",
        "        rouge_scores1 = 0\n",
        "        rouge_scores2 = 0\n",
        "        rouge_scores3 = 0\n",
        "        bleu_scoreA = 0\n",
        "\n",
        "        for i in bleu_scoreS:\n",
        "            bleu_scoreA += i\n",
        "        for i in rouge_scoreS:\n",
        "            rouge_scores1 += i[0]\n",
        "            rouge_scores2 += i[1]\n",
        "            rouge_scores3 += i[2]\n",
        "\n",
        "        print(\"\\nAverageBLEU Score:\", bleu_scoreA)\n",
        "        print(f\"Average ROUGE Scores for {count} samples\")\n",
        "        print(\"\\tAverage ROUGE-1 F1 Score:\", rouge_scores1 / count)\n",
        "        print(\"\\tAverage ROUGE-2 F1 Score:\", rouge_scores2 / count)\n",
        "        print(\"\\tAverageROUGE-L F1 Score:\", rouge_scores3 / count)\n",
        "\n",
        "    print(\"\\n ---------------------------------------------------------------\")\n",
        "\n",
        "\n",
        "Evaluate(Sample=train_dataset, count=3)  # one sample evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0oBkpjyuFZlu"
      },
      "source": [
        " ## **7. Intergrating  Retreival-Augmented Generation**\n",
        "\n",
        "\n",
        "\"Retrieval-Augmented Generation\" is a technique used in natural language processing (NLP) and artificial intelligence (AI) that combines elements of both retrieval-based and generation-based models to improve text generation tasks.\n",
        "\n",
        "In traditional text generation tasks, such as language modeling or dialogue generation, the model generates responses solely based on the input context without accessing external knowledge. However, retrieval-augmented generation introduces a retrieval step where the model first retrieves relevant information from a large external knowledge source, such as a database or a corpus of documents, before generating the output.\n",
        "\n",
        "It allows models to leverage external knowledge sources to enhance the quality and relevance of generated text, leading to more accurate and informative outputs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r4f4FrLhGJ7V",
        "outputId": "2a49d097-ecd2-4a3a-946d-9ccbe0416836"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gradient_haystack==0.2.0\n",
            "  Downloading gradient_haystack-0.2.0-py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: gradientai>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from gradient_haystack==0.2.0) (1.13.0)\n",
            "Requirement already satisfied: haystack-ai in /usr/local/lib/python3.10/dist-packages (from gradient_haystack==0.2.0) (2.2.1)\n",
            "Requirement already satisfied: aenum>=3.1.11 in /usr/local/lib/python3.10/dist-packages (from gradientai>=1.4.0->gradient_haystack==0.2.0) (3.1.15)\n",
            "Requirement already satisfied: pydantic<3,>=1.10.15 in /usr/local/lib/python3.10/dist-packages (from gradientai>=1.4.0->gradient_haystack==0.2.0) (2.7.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from gradientai>=1.4.0->gradient_haystack==0.2.0) (2.8.2)\n",
            "Requirement already satisfied: urllib3>=1.25.3 in /usr/local/lib/python3.10/dist-packages (from gradientai>=1.4.0->gradient_haystack==0.2.0) (2.0.7)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from haystack-ai->gradient_haystack==0.2.0) (3.1.4)\n",
            "Requirement already satisfied: lazy-imports in /usr/local/lib/python3.10/dist-packages (from haystack-ai->gradient_haystack==0.2.0) (0.3.1)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.10/dist-packages (from haystack-ai->gradient_haystack==0.2.0) (10.1.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from haystack-ai->gradient_haystack==0.2.0) (3.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from haystack-ai->gradient_haystack==0.2.0) (1.25.2)\n",
            "Requirement already satisfied: openai>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from haystack-ai->gradient_haystack==0.2.0) (1.33.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from haystack-ai->gradient_haystack==0.2.0) (2.0.3)\n",
            "Requirement already satisfied: posthog in /usr/local/lib/python3.10/dist-packages (from haystack-ai->gradient_haystack==0.2.0) (3.5.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from haystack-ai->gradient_haystack==0.2.0) (6.0.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from haystack-ai->gradient_haystack==0.2.0) (2.32.3)\n",
            "Requirement already satisfied: tenacity in /usr/local/lib/python3.10/dist-packages (from haystack-ai->gradient_haystack==0.2.0) (8.3.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from haystack-ai->gradient_haystack==0.2.0) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from haystack-ai->gradient_haystack==0.2.0) (4.12.1)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai>=1.1.0->haystack-ai->gradient_haystack==0.2.0) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai>=1.1.0->haystack-ai->gradient_haystack==0.2.0) (1.7.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai>=1.1.0->haystack-ai->gradient_haystack==0.2.0) (0.27.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai>=1.1.0->haystack-ai->gradient_haystack==0.2.0) (1.3.1)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.10.15->gradientai>=1.4.0->gradient_haystack==0.2.0) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.10.15->gradientai>=1.4.0->gradient_haystack==0.2.0) (2.18.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->gradientai>=1.4.0->gradient_haystack==0.2.0) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->haystack-ai->gradient_haystack==0.2.0) (2.1.5)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->haystack-ai->gradient_haystack==0.2.0) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->haystack-ai->gradient_haystack==0.2.0) (2024.1)\n",
            "Requirement already satisfied: monotonic>=1.5 in /usr/local/lib/python3.10/dist-packages (from posthog->haystack-ai->gradient_haystack==0.2.0) (1.6)\n",
            "Requirement already satisfied: backoff>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from posthog->haystack-ai->gradient_haystack==0.2.0) (2.2.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->haystack-ai->gradient_haystack==0.2.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->haystack-ai->gradient_haystack==0.2.0) (3.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->haystack-ai->gradient_haystack==0.2.0) (2024.6.2)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai>=1.1.0->haystack-ai->gradient_haystack==0.2.0) (1.2.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai>=1.1.0->haystack-ai->gradient_haystack==0.2.0) (1.0.5)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai>=1.1.0->haystack-ai->gradient_haystack==0.2.0) (0.14.0)\n",
            "Installing collected packages: gradient_haystack\n",
            "  Attempting uninstall: gradient_haystack\n",
            "    Found existing installation: gradient-haystack 0.4.0\n",
            "    Uninstalling gradient-haystack-0.4.0:\n",
            "      Successfully uninstalled gradient-haystack-0.4.0\n",
            "Successfully installed gradient_haystack-0.2.0\n"
          ]
        }
      ],
      "source": [
        "!pip install gradient_haystack==0.2.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kE3KAEc6GLVK"
      },
      "outputs": [],
      "source": [
        "from gradient_haystack.embedders.gradient_document_embedder import GradientDocumentEmbedder\n",
        "from gradient_haystack.embedders.gradient_text_embedder import GradientTextEmbedder\n",
        "from gradient_haystack.generator.base import GradientGenerator\n",
        "from haystack import Document, Pipeline\n",
        "from haystack.components.writers import DocumentWriter\n",
        "from haystack.document_stores.in_memory.document_store import InMemoryDocumentStore\n",
        "from haystack.components.retrievers.in_memory.embedding_retriever import InMemoryEmbeddingRetriever\n",
        "from haystack.components.builders import PromptBuilder\n",
        "from haystack.components.builders.answer_builder import AnswerBuilder\n",
        "import os\n",
        "import requests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x4rNiS8FGPXd"
      },
      "outputs": [],
      "source": [
        "os.environ['GRADIENT_ACCESS_TOKEN'] = \"Y2XG6OMr6oZbZETWKkPKogmWshFlD5ft\"\n",
        "os.environ['GRADIENT_WORKSPACE_ID'] = \"c571b959-4ce8-474a-b3f3-398c7b347c57_workspace\"\n",
        "\n",
        "fine_tuned_Model_Id = \"2e7a849b-239e-44a1-a88a-3be1d77c1f93_model_adapter\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HSljEN8TIEaK",
        "outputId": "828bdfec-3714-48c7-b05d-26b79cabcac0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "75697\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00,  2.77it/s]\n"
          ]
        }
      ],
      "source": [
        "document_store = InMemoryDocumentStore()\n",
        "writer = DocumentWriter(document_store=document_store)\n",
        "\n",
        "\n",
        "document_embedder = GradientDocumentEmbedder(\n",
        "    access_token=os.environ[\"GRADIENT_ACCESS_TOKEN\"],\n",
        "    workspace_id=os.environ[\"GRADIENT_WORKSPACE_ID\"],\n",
        ")\n",
        "\n",
        "# URL of the online repository where the Raw_Text_Data.txt file is located\n",
        "url = \"https://raw.githubusercontent.com/swafey-karanja/Model-training/main/Raw_Text_Data.txt\"\n",
        "\n",
        "# Send a GET request to download the file\n",
        "response = requests.get(url)\n",
        "\n",
        "# Check if the request was successful (status code 200)\n",
        "if response.status_code == 200:\n",
        "    # Read the contents of the downloaded file\n",
        "    text_data = response.text\n",
        "else:\n",
        "    # If the request was not successful, print an error message\n",
        "    print(\"Failed to download the file from the URL:\", url)\n",
        "\n",
        "docs = [\n",
        "    Document(content=text_data)\n",
        "]\n",
        "\n",
        "print(len(text_data))\n",
        "\n",
        "indexing_pipeline = Pipeline()\n",
        "indexing_pipeline.add_component(instance=document_embedder, name=\"document_embedder\")\n",
        "indexing_pipeline.add_component(instance=writer, name=\"writer\")\n",
        "indexing_pipeline.connect(\"document_embedder\", \"writer\")\n",
        "indexing_pipeline.run({\"document_embedder\": {\"documents\": docs}})\n",
        "\n",
        "text_embedder = GradientTextEmbedder(\n",
        "    access_token=os.environ[\"GRADIENT_ACCESS_TOKEN\"],\n",
        "    workspace_id=os.environ[\"GRADIENT_WORKSPACE_ID\"],\n",
        ")\n",
        "\n",
        "generator = GradientGenerator(\n",
        "    access_token=os.environ[\"GRADIENT_ACCESS_TOKEN\"],\n",
        "    workspace_id=os.environ[\"GRADIENT_WORKSPACE_ID\"],\n",
        "    model_adapter_id=fine_tuned_Model_Id,\n",
        "    max_generated_token_count=350,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FO4BvUHpIrxp"
      },
      "outputs": [],
      "source": [
        "prompt = \"\"\"You are helpful assistant meant to answer questions relating to animal husbandry. Answer the query, based on the\n",
        "content in the documents. if you dont know the answer respond by saying you are unable to assist with that at the moment.\n",
        "{{documents}}\n",
        "Query: {{query}}\n",
        "\\nAnswer:\n",
        "\"\"\"\n",
        "\n",
        "retriever = InMemoryEmbeddingRetriever(document_store=document_store)\n",
        "prompt_builder = PromptBuilder(template=prompt)\n",
        "\n",
        "rag_pipeline = Pipeline()\n",
        "rag_pipeline.add_component(instance=text_embedder, name=\"text_embedder\")\n",
        "rag_pipeline.add_component(instance=retriever, name=\"retriever\")\n",
        "rag_pipeline.add_component(instance=prompt_builder, name=\"prompt_builder\")\n",
        "rag_pipeline.add_component(instance=generator, name=\"generator\")\n",
        "rag_pipeline.add_component(instance=AnswerBuilder(), name=\"answer_builder\")\n",
        "rag_pipeline.connect(\"generator.replies\", \"answer_builder.replies\")\n",
        "rag_pipeline.connect(\"retriever\", \"answer_builder.documents\")\n",
        "rag_pipeline.connect(\"text_embedder\", \"retriever\")\n",
        "rag_pipeline.connect(\"retriever\", \"prompt_builder.documents\")\n",
        "rag_pipeline.connect(\"prompt_builder\", \"generator\")\n",
        "\n",
        "\n",
        "def LLM_Run(question):\n",
        "    result = rag_pipeline.run(\n",
        "        {\n",
        "            \"text_embedder\": {\"text\": question},\n",
        "            \"prompt_builder\": {\"query\": question},\n",
        "            \"answer_builder\": {\"query\": question}\n",
        "        }\n",
        "    )\n",
        "    return result[\"answer_builder\"][\"answers\"][0].data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DXlb_oNaItUz",
        "outputId": "3d24ca8c-2ab0-4d02-c4d2-bfb9d7f6289e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No, bulls do not show signs of Trichomoniasis.\n"
          ]
        }
      ],
      "source": [
        "Query = \"Do bulls show signs of Trichomoniasis?\"\n",
        "print(LLM_Run(Query))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 426
        },
        "id": "TqhOWDwomTYv",
        "outputId": "91708b52-7828-4a1b-89ec-81ba092d5b0b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting anvil-uplink\n",
            "  Downloading anvil_uplink-0.4.2-py2.py3-none-any.whl (90 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/90.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.1/90.1 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting argparse (from anvil-uplink)\n",
            "  Downloading argparse-1.4.0-py2.py3-none-any.whl (23 kB)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.10/dist-packages (from anvil-uplink) (0.18.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from anvil-uplink) (1.16.0)\n",
            "Collecting ws4py (from anvil-uplink)\n",
            "  Downloading ws4py-0.5.1.tar.gz (51 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.4/51.4 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: ws4py\n",
            "  Building wheel for ws4py (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ws4py: filename=ws4py-0.5.1-py3-none-any.whl size=45228 sha256=eb23a86c737c3cc6d4f4a99fd9df1f79111d833213d48a13ba3342b4c2010974\n",
            "  Stored in directory: /root/.cache/pip/wheels/2e/7c/ad/d9c746276bf024d44296340869fcb169f1e5d80fb147351a57\n",
            "Successfully built ws4py\n",
            "Installing collected packages: ws4py, argparse, anvil-uplink\n",
            "Successfully installed anvil-uplink-0.4.2 argparse-1.4.0 ws4py-0.5.1\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "id": "af179c19035c4ed1a108f277f156aa7b",
              "pip_warning": {
                "packages": [
                  "argparse",
                  "google"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "!pip install anvil-uplink"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P4_bgYoymwmX"
      },
      "outputs": [],
      "source": [
        "import anvil.server"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lk11zvaqm6Gz",
        "outputId": "0083ac16-59b5-4c1d-e1ef-d81c125f2499"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Connecting to wss://anvil.works/uplink\n",
            "Anvil websocket open\n",
            "Connected to \"Default Environment\" as SERVER\n"
          ]
        }
      ],
      "source": [
        "anvil.server.connect(\"server_5A33FQHS5NVTR2BFISKONOLE-UOGPLND5WG64J4TQ\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "umew5Tuor32l"
      },
      "outputs": [],
      "source": [
        "@anvil.server.callable\n",
        "def question_answer(question):\n",
        "  response = LLM_Run(question)\n",
        "  print(response)\n",
        "  return response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "jzQEr0QXuSyp",
        "outputId": "d85bc2ca-26e9-4fea-dc62-a23b54945ce5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The issue could be warts or calluses.\n",
            "Lumps on the skin of cattle can be caused by various factors such as insect bites, skin infections, or tumors. It is essential to monitor the lumps for changes in size, shape, or texture. If the lumps are large, painful, or appear to be growing, it is recommended to seek professional help. A veterinarian can diagnose the cause of the lumps and provide appropriate treatment.\n"
          ]
        }
      ],
      "source": [
        "anvil.server.wait_forever()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}