# -*- coding: utf-8 -*-
"""FourthYearProject.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1UuxQ9CS2S2pmJZea8RczlZbcPjuEdSD9

# **VET-ASSISTANT LLM**

**Installing dependencies**
"""

import json
import requests
import os
from gradientai import Gradient
from langchain.chains import LLMChain
from langchain.llms import GradientLLM
from langchain.prompts import PromptTemplate
import re
from nltk.translate.bleu_score import corpus_bleu
from rouge import Rouge
from gradient_haystack.embedders \
    .gradient_document_embedder import GradientDocumentEmbedder

from gradient_haystack.embedders \
    .gradient_text_embedder import GradientTextEmbedder
from gradient_haystack.generator.base import GradientGenerator
from haystack import Document, Pipeline
from haystack.components.writers import DocumentWriter
from haystack.document_stores.\
    in_memory.document_store import InMemoryDocumentStore
from haystack.components.retrievers.\
    in_memory.embedding_retriever import InMemoryEmbeddingRetriever
from haystack.components.builders import PromptBuilder
from haystack.components.builders.answer_builder import AnswerBuilder

# !pip install gradientai --upgrade
# !pip install langchain
# !pip install -U gradient_haystack
# !pip install regex
# !pip install rouge
# !pip install nltk
# !pip install wandb
# !pip install datasets
# !pip install rouge
# !pip install nltk
# !pip install langchain_community

"""## **1.Loading the Dataset**"""


# URL of the online repository where the dataset is hosted
dataset_url = "https://raw.githubusercontent.com/swafey-karanja/"
"Model-training/main/NewData.json"

# Make an HTTP GET request to fetch the dataset
response = requests.get(dataset_url)

# Check if the request was successful (status code 200)
if response.status_code == 200:
    # Load the dataset from the response content
    train_dataset = json.loads(response.text)

    # Print the size of the dataset
    print("Dataset Size:", len(train_dataset))
else:
    # Print an error message if the request failed
    print("Failed to fetch dataset. Status code:", response.status_code)

"""**Break the data into batches**"""


def divide_into_Batches(number, chunk_size):
    # Define a function to divide a number into chunks of a given size
    Batches = []
    while number > 0:
        if number >= chunk_size:
            Batches.append(chunk_size)
            number -= chunk_size
        else:
            Batches.append(number)
            break

    return Batches


Batches = divide_into_Batches(len(train_dataset), 100)
# Divide the dataset into chunks of 100 samples each
print("Batches size")
print(Batches)

"""## **2.Load the Base Model.**

Loading the Nous-Hermes-Llama2-13b base model
"""


os.environ['GRADIENT_ACCESS_TOKEN'] = "4RkXwcXCIhjSilcrkYNanvSI8h1WWrgt"
os.environ['GRADIENT_WORKSPACE_ID'] = "496b8f01-47f9-4f62-91c8-"
"e634679ca2d3_workspace"

gradient = Gradient()

print("Available Base Models")
for i in gradient.list_models(only_base=True):
    print("\t", i)

base_model_id = "NousResearch/Nous-Hermes-Llama2-13b"
base_model_name = "nous-hermes2"
base_model = gradient.get_base_model(base_model_slug="nous-hermes2")
# base model Nous-Hermes-Llama2-13b

print("\nBase Model Chosen :", base_model)

"""##**3. Creating a Model Adapter**

* Adapters are small, lightweight modules inserted between
existing layers of a pre-trained LLM. They act like "add-ons" that focus on
learning task-specific information without modifying the core knowledge
captured in the original model.

* The addapter server as the object that we are going to fine tune
"""

# our_finetune_model_name="Llama2-13b/Chatbot"
# Fine_Tune__adapter = base_model.create_model_adapter(
#         name=our_finetune_model_name, # base mode nous hermis
#         learning_rate=0.00005, #  Determines how fast a model updates its
#         knowledge during fine-tuning.
#         rank=8,  # Dimensionality

#     )

# # default hyperparameters Frozen
# hyperparameters = {
#                   "block_size": 1024,
#                   "model_max_length": 2048,
#                   "padding": "right",
#                   "use_flash_attention_2": False,
#                   "disable_gradient_checkpointing": False,
#                   "logging_steps": -1,
#                   "evaluation_strategy": "epoch",
#                   "save_total_limit": 1,
#                   "save_strategy": "epoch",
#                   "auto_find_batch_size": False,
#                   "mixed_precision": "fp16",
#                   "epochs": 3,
#                   "batch_size": 100,
#                   "warmup_ratio": 0.1,
#                   "gradient_accumulation": 1,
#                   "optimizer": "adamw_torch",
#                   "scheduler": "linear",
#                   "weight_decay": 0,
#                   "max_grad_norm": 1,
#                   "seed": 42,
#                   "apply_chat_template": False,
#                   "quantization": "int4",
#                   "target_modules": "",
#                   "merge_adapter": False,
#                   "peft": True,
#                   "lora_r": 16,
#                   "lora_alpha": 32,
#                   "lora_dropout": 0.05
#   }


# print(f"Base model id                : {Fine_Tune__adapter._base_model_id}")
# print(f"Fine tune model Name         : { Fine_Tune__adapter.name}")
# print(f"Fine tune model adapter id   : {Fine_Tune__adapter.id}")

# print("\n\n")
# print("Size of object in memory, in bytes.",
# Fine_Tune__adapter.__format__.__sizeof__()) # Size of object in memory,
# in bytes.
# Fine_Tune__adapter.__dict__

# """## **4. Fine Tuning The Adaptor**

# For this case we will be performing Laura-based finetuning which means that
# we are freezing about 99% of the layers and then finetuning an adapter on
# top of it.

# LoRA: Low-Rank Adaptation of Large Language Models is a novel technique
# introduced by Microsoft researchers to deal with the problem of fine-tuning
# large-language models with billions of parameters

#  LoRA proposes to freeze pre-trained model weights and inject trainable
# layers (rank-decomposition matrices) in each transformer block. This greatly
# reduces the number of trainable parameters and GPU memory requirements since
# gradients don't need to be computed for most model weights


# why LoRA finetuning:

# 1. Faster Training: Since only the added task-specific layers are trained
# while the pre-trained model's parameters remain frozen, the fine-tuning
# process is generally faster compared to training a model from scratch
# 2. Computation requirements are lower. We could create a full fine-tuned
# model in a 2080 Ti with 11 GB of VRAM!
# 3. Trained weights are  much smaller. Because the original model is frozen
# and we inject new layers to be trained

#  [for more info](https://huggingface.co/blog/lora)
# """

# print(f"Our Model id:  {Fine_Tune__adapter.id}")
# num_epochs = 1  # num_epochs is the number of times you fine-tune the model
# # more epochs tends to get better results, but you also run the risk of
# "overfitting"
# count = 0
# print("================================================================\n")
# print("Fine tuning . . .\n")
# while count < num_epochs:
#     print(f"Fine-tuning the model, iteration {count + 1}")
#     s = 0
#     n = 1
#     for Batch in Batches:
#         print(f"Batch {n} range: {s} : {(s + Batch)}")

#         # Try to fine-tune the model with the chunk of samples,
#         while True:
#             try:
#                 metric = Fine_Tune__adapter.fine_tune(
#                           samples=train_dataset[s: s + Batch])
#                 print(f"\t Batch {n} Evaluation :", metric)
#                 break
#             except:
#                 pass


#         s += Batch
#         n += 1
#     count = count + 1

"""## **5.Model Inference**

"model inference" typically refers to the process of using a trained model to
make predictions on new, unseen data. Fine-tuning a model involves taking a
pre-trained model and further training it on a specific task or dataset to
improve its performance.

When fine-tuning a model, the process of inference remains the same as with
any other trained model. Once the fine-tuning is complete, you can use the
model to make predictions on new data by passing the data through the model
and obtaining the output.
"""


Fine_Tune__adapter_ID = "28643f93-bdd5-4602-b911-2e9fea183186_model_adapter"
#  Fine_Tune__adapter_ID = Fine_Tune__adapter.id
#  creating a GradientLLM object
llm = GradientLLM(
    model=Fine_Tune__adapter_ID,
    model_kwargs=dict(
        max_generated_token_count=128,
        # Adjust how your model generates completions
        temperature=0.7,  # randomness
        top_k=50  # Restricts the model to pick from k most likely words,
    ),
)

"""### **Formatting Prompts**

The model follows the Alpaca prompt format which provides a structured way to
input information to the model, guiding it on what kind of output is desired.
"""

template = """### Instruction: {Instruction} \n\n### Response:"""

prompt = PromptTemplate(template=template, input_variables=["Instruction"])

llm_chain = LLMChain(prompt=prompt, llm=llm)

"""#### *Example* *inputs* *and* *corresponding* *outputs*"""

Question1 = "How does Clostridial Diseases impact milk production, "
"reproductive performance, and overall well-being in cows?"
Question2 = "what are some of the risk factors associated with lameness in "
"dairy cows?"
Question3 = "How dose lameness impact mill production, reproduction and "
"overall well-being in cows"
Question4 = "What diseases are prevelant in dairy small ruminant, and what "
"managment practice can mitigate their impact "
Question5 = "what specific health managment strategies should be implemented "
"to prevent or treat common cow diseases?"


print("Question :\n", Question1)
Answer = llm_chain.run(Instruction=f"{Question1}")
print("Answer :\n", Answer, "\n\n")

print("Question :\n", Question2)
Answer = llm_chain.run(Instruction=f"{Question2}")
print("Answer :\n", Answer, "\n\n")

print("Question :\n", Question3)
Answer = llm_chain.run(Instruction=f"{Question3}")
print("Answer :\n", Answer, "\n\n")

print("Question :\n", Question4)
Answer = llm_chain.run(Instruction=f"{Question4}")
print("Answer :\n", Answer, "\n\n")

print("Question :\n", Question5)
Answer = llm_chain.run(Instruction=f"{Question5}")
print("Answer :\n", Answer, "\n\n")

"""## **6.Model Evaluation**

Here we are using two popular automatic evaluation metrics to assess the
performance of your LLM:

* BLEU score: This metric calculates the n-gram precision between the
generated response and the reference response

* ROUGE score: This metric measures the overlap in word n-grams and longest
common subsequences between the generated response and the reference response.

BLEU and ROUGE scores are calculated to compare the generated response with
the target response.
"""


def compute_rouge_scores(hypotheses, references):
    rouge = Rouge()
    scores = rouge.get_scores(hypotheses, references, avg=True)
    return scores


def compute_bleu_score(target_response, llm_responses):
    bleu_score = corpus_bleu([target_response.split()],
                             [llm_responses.split()])  # Calculate BLEU score
    return bleu_score


def Find_Instruction(input_pattern, input_string):
    matches = re.findall(input_pattern, input_string, re.DOTALL)

    # If there are matches, extract the first one
    extracted_string = None
    if matches:
        extracted_string = matches[0]

    return extracted_string


def Evaluate(Sample=None, count=0):
    print("\n =================================== "
          "Evaluation =================================== ")
    input_pattern = r'<s>### Instruction:\n(.*?) \n'
    response_pattern = r'Response:\n(.*?)</s>'
    bleu_scoreS = []
    rouge_scoreS = []

    if count != 0:
        iteration = count - 1
    else:
        iteration = count

    while iteration >= 0:

        input_query = Find_Instruction(
            input_pattern, Sample[iteration]["inputs"])
        target_response = Find_Instruction(
            response_pattern, Sample[iteration]["inputs"])

        if input_query and target_response is not None:
            print("\n --------------------------------"
                  "-------------------------------")
            print("INPUT QUERY:\n", input_query)
            print("\nTARGET RESPONSE:\n", target_response)

            llm_responses = llm_chain.run(Instruction=f"{input_query}")
            print("\nLLM RESPONSE:\n", llm_responses)

            rouge_scores = compute_rouge_scores(llm_responses, target_response)

            bleu_score = compute_bleu_score(target_response, llm_responses)
            print("\nBLEU Score:", bleu_score)
            print("ROUGE Scores:")
            print("\tROUGE-1 F1 Score:", rouge_scores["rouge-1"]["f"])
            print("\tROUGE-2 F1 Score:", rouge_scores["rouge-2"]["f"])
            print("\tROUGE-L F1 Score:", rouge_scores["rouge-l"]["f"])
            rouge_scoreS.append((rouge_scores["rouge-1"]["f"],
                                 rouge_scores["rouge-2"]["f"],
                                 rouge_scores["rouge-l"]["f"]))
            bleu_scoreS.append(bleu_score)

        iteration -= 1

    if count > 0:
        rouge_scores1 = 0
        rouge_scores2 = 0
        rouge_scores3 = 0
        bleu_scoreA = 0

        for i in bleu_scoreS:
            bleu_scoreA += i
        for i in rouge_scoreS:
            rouge_scores1 += i[0]
            rouge_scores2 += i[1]
            rouge_scores3 += i[2]

        print("\nAverageBLEU Score:", bleu_scoreA)
        print(f"Average ROUGE Scores for {count} samples")
        print("\tAverage ROUGE-1 F1 Score:", rouge_scores1 / count)
        print("\tAverage ROUGE-2 F1 Score:", rouge_scores2 / count)
        print("\tAverageROUGE-L F1 Score:", rouge_scores3 / count)

    print("\n ---------------------------------------------------------------")


Evaluate(Sample=train_dataset, count=3)  # one sample evaluation

""" ## **7. Intergrating  Retreival-Augmented Generation**


"Retrieval-Augmented Generation" is a technique used in natural language
processing (NLP) and artificial intelligence (AI) that combines elements of
both retrieval-based and generation-based models to improve text generation
tasks.

In traditional text generation tasks, such as language modeling or dialogue
generation, the model generates responses solely based on the input context
without accessing external knowledge. However, retrieval-augmented generation
introduces a retrieval step where the model first retrieves relevant
information from a large external knowledge source, such as a database or a
corpus of documents, before generating the output.

It allows models to leverage external knowledge sources to enhance the quality
and relevance of generated text, leading to more accurate and informative
outputs.
"""

# !pip install gradient_haystack==0.2.0


os.environ['GRADIENT_ACCESS_TOKEN'] = "4RkXwcXCIhjSilcrkYNanvSI8h1WWrgt"
os.environ['GRADIENT_WORKSPACE_ID'] = "496b8f01-47f9-4f62-91c8-"
"e634679ca2d3_workspace"

fine_tuned_Model_Id = "28643f93-bdd5-4602-b911-2e9fea183186_model_adapter"

document_store = InMemoryDocumentStore()
writer = DocumentWriter(document_store=document_store)


document_embedder = GradientDocumentEmbedder(
    access_token=os.environ["GRADIENT_ACCESS_TOKEN"],
    workspace_id=os.environ["GRADIENT_WORKSPACE_ID"],
)

# URL of the online repository where the Raw_Text_Data.txt file is located
url = "https://raw.githubusercontent.com/swafey-karanja/"
"Model-training/main/Raw_Text_Data.txt"

# Send a GET request to download the file
response = requests.get(url)

# Check if the request was successful (status code 200)
if response.status_code == 200:
    # Read the contents of the downloaded file
    text_data = response.text
else:
    # If the request was not successful, print an error message
    print("Failed to download the file from the URL:", url)

docs = [
    Document(content=text_data)
]

print(len(text_data))

indexing_pipeline = Pipeline()
indexing_pipeline.add_component(
    instance=document_embedder, name="document_embedder")
indexing_pipeline.add_component(instance=writer, name="writer")
indexing_pipeline.connect("document_embedder", "writer")
indexing_pipeline.run({"document_embedder": {"documents": docs}})

text_embedder = GradientTextEmbedder(
    access_token=os.environ["GRADIENT_ACCESS_TOKEN"],
    workspace_id=os.environ["GRADIENT_WORKSPACE_ID"],
)

generator = GradientGenerator(
    access_token=os.environ["GRADIENT_ACCESS_TOKEN"],
    workspace_id=os.environ["GRADIENT_WORKSPACE_ID"],
    model_adapter_id=fine_tuned_Model_Id,
    max_generated_token_count=350,
)

prompt = """You are helpful assistant meant to answer questions relating to
animal husbandry. Answer the query, based on the
content in the documents. if you dont know the answer respond by saying you
are unable to assist with that at the moment.
{{documents}}
Query: {{query}}
\nAnswer:
"""

retriever = InMemoryEmbeddingRetriever(document_store=document_store)
prompt_builder = PromptBuilder(template=prompt)

rag_pipeline = Pipeline()
rag_pipeline.add_component(instance=text_embedder, name="text_embedder")
rag_pipeline.add_component(instance=retriever, name="retriever")
rag_pipeline.add_component(instance=prompt_builder, name="prompt_builder")
rag_pipeline.add_component(instance=generator, name="generator")
rag_pipeline.add_component(instance=AnswerBuilder(), name="answer_builder")
rag_pipeline.connect("generator.replies", "answer_builder.replies")
rag_pipeline.connect("retriever", "answer_builder.documents")
rag_pipeline.connect("text_embedder", "retriever")
rag_pipeline.connect("retriever", "prompt_builder.documents")
rag_pipeline.connect("prompt_builder", "generator")


def LLM_Run(question):
    result = rag_pipeline.run(
        {
            "text_embedder": {"text": question},
            "prompt_builder": {"query": question},
            "answer_builder": {"query": question}
        }
    )
    return result["answer_builder"]["answers"][0].data


Query = "Do bulls show signs of Trichomoniasis?"
print(LLM_Run(Query))

# !pip install anvil-uplink

# import anvil.server

# anvil.server.connect("server_5A33FQHS5NVTR2BFISKONOLE-UOGPLND5WG64J4TQ")

# @anvil.server.callable
# def question_answer(question):
#   response = LLM_Run(question)
#   print(response)
#   return response

# anvil.server.wait_forever()
